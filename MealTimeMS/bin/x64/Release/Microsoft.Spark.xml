<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.Spark</name>
    </assembly>
    <members>
        <member name="T:Microsoft.Spark.SinceAttribute">
            <summary>
            Custom attribute to denote the Spark version in which an API is introduced.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SinceAttribute.#ctor(System.String)">
            <summary>
            Constructor for SinceAttribute class.
            </summary>
            <param name="version">Spark version</param>
        </member>
        <member name="P:Microsoft.Spark.SinceAttribute.Version">
            <summary>
            Returns the Spark version.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.IJvmBridge">
            <summary>
            Interface of the bridge between JVM and CLR.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.JsonSerDe">
            <summary>
            Json.NET Serialization/Deserialization helper class.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JsonSerDe.SortProperties(Newtonsoft.Json.Linq.JObject)">
            Note: Scala side uses JSortedObject when parsing JSON, so the properties
            in JObject need to be sorted.
            <summary>
            Extension method to sort items in a JSON object by keys.
            </summary>
            <param name="jObject"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JsonSerDe.SortProperties(Newtonsoft.Json.Linq.JArray)">
            <summary>
            Extend method to sort items in a JSON array by keys.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.JvmBridge">
            <summary>
            Implementation of thread safe IPC bridge between JVM and CLR
            Using a concurrent socket connection queue (lightweight synchronization mechanism)
            supporting async JVM calls like StreamingContext.AwaitTermination()
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.JvmObjectId">
            <summary>
            JvmObjectId represents the unique owner for a JVM object.
            The reason for having another layer on top of string id is
            so that JvmObjectReference can be copied.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectId.#ctor(System.String,Microsoft.Spark.Interop.Ipc.IJvmBridge)">
            <summary>
            Constructor for JvmObjectId class.
            </summary>
            <param name="id">Unique identifier</param>
            <param name="jvm">JVM bridge object</param>
        </member>
        <member name="P:Microsoft.Spark.Interop.Ipc.JvmObjectId.Id">
            <summary>
            An unique identifier for an object created on the JVM.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Interop.Ipc.JvmObjectId.Jvm">
            <summary>
            JVM bridge object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectId.op_Implicit(Microsoft.Spark.Interop.Ipc.JvmObjectId)~System.String">
            <summary>
            Implicit conversion to string.
            </summary>
            <param name="jvmObjectId">JvmObjectId to convert from</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectId.ToString">
            <summary>
            Returns the string version of this object which is the unique id
            of the JVM object.
            </summary>
            <returns>Id of the JVM object</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectId.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object by comparing the id.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal.</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectId.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.IJvmObjectReferenceProvider">
            <summary>
            Implemented by objects that contain a <see cref="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference"/>.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Interop.Ipc.IJvmObjectReferenceProvider.Reference">
            <summary>
            Gets the <see cref="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference"/> wrapped by the object.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference">
            <summary>
            Reference to object created in JVM.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.JvmObjectReference._creationTime">
            <summary>
            The time when this reference was created.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.#ctor(System.String,Microsoft.Spark.Interop.Ipc.IJvmBridge)">
            <summary>
            Constructor for the JvmObjectReference class.
            </summary>
            <param name="id">Id for the JVM object</param>
            <param name="jvm">IJvmBridge instance that created the JVM object</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Copy constructor.
            </summary>
            <param name="other">Other JvmObjectReference object to copy from.</param>
        </member>
        <member name="P:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Id">
            <summary>
            An unique identifier for an object created on the JVM.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Jvm">
            <summary>
            IJvmBridge instance that created the JVM object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Invoke(System.String,System.Object)">
            <summary>
            Invokes a method on the JVM object that this JvmObjectReference references to.
            </summary>
            <param name="arg0">Parameter for the method.</param>
            <param name="methodName">Method name to invoke</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Invoke(System.String,System.Object,System.Object)">
            <summary>
            Invokes a method on the JVM object that this JvmObjectReference references to.
            </summary>
            <param name="arg0">First parameter for the method.</param>
            <param name="arg1">Second parameter for the method.</param>
            <param name="methodName">Method name to invoke</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Invoke(System.String,System.Object[])">
            <summary>
            Invokes a method on the JVM object that this JvmObjectReference references to.
            </summary>
            <param name="methodName">Method name to invoke</param>
            <param name="args">Parameters for the method</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.ToString">
            <summary>
            Returns the string version of this object which is the unique id
            of the JVM object.
            </summary>
            <returns>Id of the JVM object</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object by comparing the ids.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal.</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.GetDebugInfo">
            <summary>
            Gets the debug info on the JVM object that the current object refers to.
            </summary>
            <returns>The debug info of the JVM object</returns>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.PayloadHelper">
            <summary>
            Helper to build the IPC payload for JVM calls from CLR.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.SpecialLengths">
            <summary>
            Enums with which Worker communicates with Spark.
            See spark/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.END_OF_DATA_SECTION">
            <summary>
            Flag to indicate the end of data section
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.PYTHON_EXCEPTION_THROWN">
            <summary>
            Flag to indicate an exception thrown from .NET side
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.TIMING_DATA">
            <summary>
            Flag to indicate a timing data
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.END_OF_STREAM">
            <summary>
            Flag to indicate the end of stream
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.NULL">
            <summary>
            Flag to indicate non-defined type
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.START_ARROW_STREAM">
            <summary>
            Flag used by PySpark only.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.SerDe">
            <summary>
            Serialization and Deserialization of data types between JVM and CLR
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadBool(System.IO.Stream)">
            <summary>
            Reads a boolean from a stream.
            </summary>
            <param name="s">The stream to read</param>
            <returns>The boolean value read from the stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadInt32(System.IO.Stream)">
            <summary>
            Reads an integer from a stream.
            </summary>
            <param name="s">The stream to be read</param>
            <returns>The integer read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadInt64(System.IO.Stream)">
            <summary>
            Reads a long integer from a stream.
            </summary>
            <param name="s">The stream to be read</param>
            <returns>The long integer read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadDouble(System.IO.Stream)">
            <summary>
            Reads a double from a stream.
            </summary>
            <param name="s">The stream to be read</param>
            <returns>The double read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadString(System.IO.Stream)">
            <summary>
            Reads a string from a stream
            </summary>
            <param name="s">The stream to be read</param>
            <returns>The string read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadString(System.IO.Stream,System.Int32)">
            <summary>
            Reads a string with a given length from a stream
            </summary>
            <param name="s">The stream to be read</param>
            <param name="length">The length to be read</param>
            <returns>The string read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadBytes(System.IO.Stream,System.Int32)">
            <summary>
            Reads a byte array with a given length from a stream
            </summary>
            <param name="s">The stream to be read</param>
            <param name="length">The length to be read</param>
            <returns>The a byte array read from stream</returns>
            <exception cref="T:System.ArgumentOutOfRangeException">
            An ArgumentOutOfRangeException thrown if the given length is negative
            </exception>
            <exception cref="T:System.ArgumentException">
            An ArgumentException if the actual read length is less than the given length
            </exception>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadBytes(System.IO.Stream)">
            <summary>
            Reads a byte array from a stream. The first 4 bytes indicate the length of a byte array.
            </summary>
            <param name="s">The stream to be read</param>
            <returns>The byte array read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Byte)">
            <summary>
            Writes a byte to a stream
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The byte to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Byte[])">
            <summary>
            Writes a byte array to a stream
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The byte array to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Byte[],System.Int32)">
            <summary>
            Writes a byte array to a stream
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The byte array to write</param>
            <param name="count">The number of bytes in the array to write.</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Boolean)">
            <summary>
            Writes a boolean to a stream
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The boolean value to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Int32)">
            <summary>
            Writes an integer to a stream (big-endian).
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The integer to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Int64)">
            <summary>
            Writes a long integer to a stream (big-endian).
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The long integer to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Double)">
            <summary>
            Writes a double to a stream (big-endian).
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The double to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.String)">
            <summary>
            Writes a string to a stream.
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The string to write</param>
        </member>
        <member name="T:Microsoft.Spark.Interop.SparkEnvironment">
            <summary>
            Contains everything needed to setup an environment for using .NET with Spark.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Network.DefaultSocketWrapper">
            <summary>
            A simple wrapper of System.Net.Sockets.Socket class.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.#ctor">
            <summary>
            Default constructor that creates a new instance of DefaultSocket class which represents
            a traditional socket (System.Net.Socket.Socket).
            
            This socket is bound to Loopback with port 0.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.#ctor(System.Net.Sockets.Socket)">
            <summary>
            Initializes a instance of DefaultSocket class using the specified
            System.Net.Socket.Socket object.
            </summary>
            <param name="socket">The existing socket</param>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.Dispose">
            <summary>
            Releases all resources used by the current instance of the DefaultSocket class.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.Accept">
            <summary>
            Accepts a incoming connection request.
            </summary>
            <returns>A DefaultSocket instance used to send and receive data</returns>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.Connect(System.Net.IPAddress,System.Int32,System.String)">
            <summary>
            Establishes a connection to a remote host that is specified by an IP address and
            a port number.
            </summary>
            <param name="remoteaddr">The IP address of the remote host</param>
            <param name="port">The port number of the remote host</param>
            <param name="secret">Secret string to use for connection</param>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.CreateNetworkStream">
            <summary>
            Returns the NetworkStream used to send and receive data.
            </summary>
            <remarks>
            GetStream returns a NetworkStream that you can use to send and receive data.
            You must close/dispose the NetworkStream by yourself. Closing DefaultSocketWrapper
            does not release the NetworkStream.
            </remarks>
            <returns>The underlying Stream instance that be used to send and receive data</returns>
        </member>
        <member name="P:Microsoft.Spark.Network.DefaultSocketWrapper.InputStream">
            <summary>
            Returns a stream used to receive data only.
            </summary>
            <returns>The underlying Stream instance that be used to receive data</returns>
        </member>
        <member name="P:Microsoft.Spark.Network.DefaultSocketWrapper.OutputStream">
            <summary>
            Returns a stream used to send data only.
            </summary>
            <returns>The underlying Stream instance that be used to send data</returns>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.Listen(System.Int32)">
            <summary>
            Starts listening for incoming connections requests
            </summary>
            <param name="backlog">The maximum length of the pending connections queue. </param>
        </member>
        <member name="P:Microsoft.Spark.Network.DefaultSocketWrapper.LocalEndPoint">
            <summary>
            Returns the local endpoint.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Network.ISocketWrapper">
            <summary>
            ISocketWrapper interface defines the common methods to operate a socket.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Network.ISocketWrapper.Accept">
            <summary>
            Accepts a incoming connection request.
            </summary>
            <returns>A ISocket instance used to send and receive data</returns>
        </member>
        <member name="M:Microsoft.Spark.Network.ISocketWrapper.Connect(System.Net.IPAddress,System.Int32,System.String)">
            <summary>
            Establishes a connection to a remote host that is specified by an IP address and
            a port number.
            </summary>
            <param name="remoteaddr">The IP address of the remote host</param>
            <param name="port">The port number of the remote host</param>
            <param name="secret">Optional secret string to use for connection</param>
        </member>
        <member name="P:Microsoft.Spark.Network.ISocketWrapper.InputStream">
            <summary>
            Returns a stream used to receive data only.
            </summary>
            <returns>The underlying Stream instance that be used to receive data</returns>
        </member>
        <member name="P:Microsoft.Spark.Network.ISocketWrapper.OutputStream">
            <summary>
            Returns a stream used to send data only.
            </summary>
            <returns>The underlying Stream instance that be used to send data</returns>
        </member>
        <member name="M:Microsoft.Spark.Network.ISocketWrapper.Listen(System.Int32)">
            <summary>
            Starts listening for incoming connections requests
            </summary>
            <param name="backlog">The maximum length of the pending connections queue</param>
        </member>
        <member name="P:Microsoft.Spark.Network.ISocketWrapper.LocalEndPoint">
            <summary>
            Returns the local endpoint.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Network.SocketFactory">
            <summary>
            SocketFactory is used to create ISocketWrapper instance.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Network.SocketFactory.CreateSocket">
            <summary>
            Creates an ISocket instance based on the socket type set.
            </summary>
            <returns>
            ISocketWrapper instance.
            </returns>
        </member>
        <member name="T:Microsoft.Spark.PairRDDFunctions">
            <summary>
            Extra functions available on RDDs of (key, value) pairs through extension methods.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.PairRDDFunctions.CollectAsMap``2(Microsoft.Spark.RDD{System.Tuple{``0,``1}})">
            <summary>
            Returns the key-value pairs in this RDD as a dictionary.
            </summary>
            <typeparam name="TKey">Type of the key</typeparam>
            <typeparam name="TValue">Type of the value</typeparam>
            <param name="self">RDD object to apply</param>
            <returns>Dictionary of RDD content</returns>
        </member>
        <member name="M:Microsoft.Spark.PairRDDFunctions.Keys``2(Microsoft.Spark.RDD{System.Tuple{``0,``1}})">
            <summary>
            Return an RDD with the keys of each tuple.
            </summary>
            <typeparam name="TKey">Type of the key</typeparam>
            <typeparam name="TValue">Type of the value</typeparam>
            <param name="self">RDD object to apply</param>
            <returns>RDD with the keys of each tuple</returns>
        </member>
        <member name="M:Microsoft.Spark.PairRDDFunctions.Values``2(Microsoft.Spark.RDD{System.Tuple{``0,``1}})">
            <summary>
            Return an RDD with the values of each tuple.
            </summary>
            <typeparam name="TKey">Type of the key</typeparam>
            <typeparam name="TValue">Type of the value</typeparam>
            <param name="self">RDD object to apply</param>
            <returns>RDD with the values of each tuple</returns>
        </member>
        <member name="T:Microsoft.Spark.RDD`1">
            <summary>
            A Resilient Distributed Dataset(RDD), the basic abstraction in Spark,
            represents an immutable, partitioned collection of elements that can be
            operated on in parallel. This class contains the basic operations available
            on all RDDs.
            </summary>
            <typeparam name="T">Type of the elements in the RDD</typeparam>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._jvmObject">
            <summary>
            The JVM object for this RDD. This can be null if the current
            RDD is not materialized.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._prevRddJvmObjRef">
            <summary>
            The previous RDD object that this RDD references to. This is
            used by <see cref="T:Microsoft.Spark.PipelinedRDD`1"/> to chain RDD operations.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._sparkContext">
            <summary>
            SparkContext object associated with the RDD.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._isCached">
            <summary>
            Flag that checks if <see cref="M:Microsoft.Spark.RDD`1.Cache"/> is called.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._isCheckpointed">
            <summary>
            Flag that checks if <see cref="M:Microsoft.Spark.RDD`1.Checkpoint"/> is called.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._serializedMode">
            <summary>
            Serialization mode for the current RDD. This will be
            translated into serialization mode while creating a serialized command.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._prevSerializedMode">
            <summary>
            Serialization mode for the previously pipelined RDD. This will be
            translated into deserialization mode while creating a serialized command.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference,Microsoft.Spark.SparkContext,Microsoft.Spark.Utils.CommandSerDe.SerializedMode)">
            <summary>
            Constructor mainly called by SparkContext for creating the first RDD
            via <see cref="M:Microsoft.Spark.SparkContext.Parallelize``1(System.Collections.Generic.IEnumerable{``0},System.Nullable{System.Int32})"/>, etc.
            </summary>
            <param name="jvmObject">The reference to the RDD JVM object</param>
            <param name="sparkContext">SparkContext object</param>
            <param name="serializedMode">Serialization mode for the current RDD</param>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference,Microsoft.Spark.SparkContext,Microsoft.Spark.Utils.CommandSerDe.SerializedMode,Microsoft.Spark.Utils.CommandSerDe.SerializedMode)">
            <summary>
            Constructor mainly called by <see cref="T:Microsoft.Spark.PipelinedRDD`1"/>.
            </summary>
            <param name="prevRddJvmObjRef">
            The reference to the RDD JVM object from which pipeline is created
            </param>
            <param name="sparkContext">SparkContext object</param>
            <param name="serializedMode">Serialization mode for the current RDD</param>
            <param name="prevSerializedMode">Serialization mode for the previous RDD</param>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Cache">
            <summary>
            Persist this RDD with the default storage level (MEMORY_ONLY).
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Checkpoint">
            <summary>
            Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint
            directory set with <see cref="M:Microsoft.Spark.SparkContext.SetCheckpointDir(System.String)"/> and all
            references to its parent RDDs will be removed. This function must be called before
            any job has been executed on this RDD. It is strongly recommended that this RDD is
            persisted in memory, otherwise saving it in a file will require re-computation.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Map``1(System.Func{`0,``0},System.Boolean)">
            <summary>
            Return a new RDD by applying a function to all elements of this RDD.
            </summary>
            <typeparam name="U">Type of the new RDD elements</typeparam>
            <param name="func">Function to apply</param>
            <param name="preservesPartitioning">Flag to preserve partitioning</param>
            <returns>New RDD by applying a given function</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.FlatMap``1(System.Func{`0,System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new RDD by first applying a function to all elements of this RDD,
            and then flattening the results.
            </summary>
            <typeparam name="U">Type of the new RDD elements</typeparam>
            <param name="func">Function to apply</param>
            <param name="preservesPartitioning">Flag to preserve partitioning</param>
            <returns>New RDD by applying a given function</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.MapPartitions``1(System.Func{System.Collections.Generic.IEnumerable{`0},System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new RDD by applying a function to each partition of this RDD.
            </summary>
            
            <remarks>
            "preservesPartitioning" indicates whether the input function preserves the
            partitioner, which should be false unless this is a pair RDD and the input
            function doesn't modify the keys.
            </remarks>
            <typeparam name="U">Type of the new RDD elements</typeparam>
            <param name="func">Function to apply</param>
            <param name="preservesPartitioning">Flag to preserve partitioning</param>
            <returns>New RDD by applying a given function</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.MapPartitionsWithIndex``1(System.Func{System.Int32,System.Collections.Generic.IEnumerable{`0},System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new RDD by applying a function to each partition of this RDD,
            while tracking the index of the original partition.
            </summary>
            <remarks>
            "preservesPartitioning" indicates whether the input function preserves the
            partitioner, which should be false unless this is a pair RDD and the input
            function doesn't modify the keys.
            </remarks>
            <typeparam name="U">Type of the new RDD elements</typeparam>
            <param name="func">Function to apply</param>
            <param name="preservesPartitioning">Flag to preserve partitioning</param>
            <returns>New RDD by applying a given function</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.GetNumPartitions">
            <summary>
            Return the number of partitions in this RDD.
            </summary>
            <returns>The number of partitions in this RDD</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Filter(System.Func{`0,System.Boolean})">
            <summary>
            Return a new RDD containing only the elements that satisfy a predicate.
            </summary>
            <param name="func">Predicate function to apply</param>
            <returns>A new RDD with elements that satisfy a predicate</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Sample(System.Boolean,System.Double,System.Nullable{System.Int64})">
            <summary>
            Return a sampled subset of this RDD with a seed.
            </summary>
            <remarks>
            This is NOT guaranteed to provide exactly the fraction of the count
            of the given RDD.
            </remarks>
            <param name="withReplacement">True if elements be sampled multiple times</param>
            <param name="fraction">
            Expected size of the sample as a fraction of this RDD's size without replacement
            </param>
            <param name="seed">Optional user-supplied seed (random seed if not provided)</param>
            <returns>A sampled subset of this RDD</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Collect">
            <summary>
            Return an enumerable collection that contains all of the elements in this RDD.
            </summary>
            <remarks>
            This method should only be used if the resulting array is expected to be small,
            as all the data is loaded into the driver's memory.
            </remarks>
            <returns>An enumerable collection of all the elements.</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.MapPartitionsWithIndexInternal``1(Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate,System.Boolean)">
            <summary>
            Helper function for creating PipelinedRDD.
            </summary>
            <typeparam name="U">Type of the new RDD elements</typeparam>
            <param name="func">Function to apply</param>
            <param name="preservesPartitioning">Flag to preserve partitioning</param>
            <returns>New RDD by applying a given function</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.CollectAndServe">
            <summary>
            Returns the socket info by calling collectAndServe on the RDD object.
            </summary>
            <returns>Socket info</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.GetJvmRef">
            <summary>
            Returns the JvmObjectReference object of this RDD.
            </summary>
            <remarks>
            It is possible that the JvmObjectReference object is null depending
            on how the RDD object is instantiated (e.g., <see cref="T:Microsoft.Spark.PipelinedRDD`1"/>). Thus,
            this function should be used instead of directly accessing _jvmObject because the
            derived class (e.g., <see cref="T:Microsoft.Spark.PipelinedRDD`1"/>) can override the behavior
            when _jvmObject is null.
            </remarks>
            <returns>JvmObjetReference object for this RDD</returns>
        </member>
        <member name="T:Microsoft.Spark.RDD`1.MapUdfWrapper`2">
            <summary>
            Helper to map the UDF for Map() to <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>.
            </summary>
            <typeparam name="TArg">Input type</typeparam>
            <typeparam name="TResult">Output type</typeparam>
        </member>
        <member name="T:Microsoft.Spark.RDD`1.FlatMapUdfWrapper`2">
            <summary>
            Helper to map the UDF for FlatMap() to <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>
            </summary>
            <typeparam name="TArg">Input type</typeparam>
            <typeparam name="TResult">Output type</typeparam>
        </member>
        <member name="T:Microsoft.Spark.RDD`1.MapPartitionsUdfWrapper`2">
            <summary>
            Helper to map the UDF for MapPartitions() to
            <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>.
            </summary>
            <typeparam name="TArg">Input type</typeparam>
            <typeparam name="TResult">Output type</typeparam>
        </member>
        <member name="T:Microsoft.Spark.RDD`1.MapPartitionsWithIndexUdfWrapper`2">
            <summary>
            Helper to map the UDF for MapPartitionsWithIndex() to
            <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>.
            </summary>
            <typeparam name="TArg">Input type</typeparam>
            <typeparam name="TResult">Output type</typeparam>
        </member>
        <member name="T:Microsoft.Spark.RDD`1.FilterUdfWrapper">
            <summary>
            Helper to map the UDF for Filter() to
            <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.PipelinedRDD`1">
            <summary>
            PipelinedRDD is used to pipeline functions applied to RDD.
            </summary>
            <typeparam name="T">Type of the elements in the RDD</typeparam>
        </member>
        <member name="M:Microsoft.Spark.PipelinedRDD`1.MapPartitionsWithIndexInternal``1(Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate,System.Boolean)">
            <summary>
            Return a new RDD by applying a function to each partition of this RDD,
            while tracking the index of the original partition.
            </summary>
            <typeparam name="U">The element type of new RDD</typeparam>
            <param name="newFunc">The function to be applied to each partition</param>
            <param name="preservesPartitioning">
            Indicates if it preserves partition parameters
            </param>
            <returns>A new RDD</returns>
        </member>
        <member name="P:Microsoft.Spark.PipelinedRDD`1.Microsoft#Spark#Interop#Ipc#IJvmObjectReferenceProvider#Reference">
            <summary>
            Returns the JVM reference for this RDD. It also initializes the reference
            if it is not yet initialized.
            
            Note that PipelineRDD uses the JavaRDD internally.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.PipelinedRDD`1.IsPipelinable">
            <summary>
            Checks whether worker functions can be pipelined.
            </summary>
            <returns>True if worker functions can be pipelined.</returns>
        </member>
        <member name="T:Microsoft.Spark.RDD.Collector">
            <summary>
            Collector collects objects from a socket.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.RDD.Collector.Collect(System.IO.Stream,Microsoft.Spark.Utils.CommandSerDe.SerializedMode)">
            <summary>
            Collects pickled row objects from the given socket.
            </summary>
            <param name="stream">Stream object to read from</param>
            <param name="serializedMode">Serialized mode for each element</param>
            <returns>Collection of row objects</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD.Collector.GetDeserializer(Microsoft.Spark.Utils.CommandSerDe.SerializedMode)">
            <summary>
            Returns a deserializer based on the given serialization mode.
            </summary>
            <param name="mode">Serialization mode</param>
            <returns>A deserializer object</returns>
        </member>
        <member name="T:Microsoft.Spark.RDD.Collector.IDeserializer">
            <summary>
            Interface to deserialize an object from a given stream.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.Collector.BinaryDeserializer">
            <summary>
            Deserializer using the BinaryFormatter.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.Collector.StringDeserializer">
            <summary>
            Deserializer for UTF-8 strings.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.WorkerFunction">
            <summary>
            WorkerFunction provides the delegate type that is used for unifying
            UDFs used for RDD. It also provides functionality to chain delegates.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate">
            <summary>
            Delegate type to which each RDD UDF is transformed.
            </summary>
            <param name="splitId">split id for the current task</param>
            <param name="input">enumerable collection of objects</param>
            <returns>enumerable collection of objects after applying UDF</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD.WorkerFunction.Chain(Microsoft.Spark.RDD.WorkerFunction,Microsoft.Spark.RDD.WorkerFunction)">
            <summary>
            Used to chain two function.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.WorkerFunction.WrokerFuncChainHelper">
            <summary>
            Helper to chain two delegates.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Services.ConfigurationService">
            <summary>
            Implementation of configuration service that helps getting config settings
            to be used in .NET backend.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.ConfigurationService.GetBackendPortNumber">
            <summary>
            Returns the port number for socket communication between JVM and CLR.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.ConfigurationService.GetWorkerExePath">
            <summary>
            Returns the worker executable path.
            </summary>
            <returns>Worker executable path</returns>
        </member>
        <member name="T:Microsoft.Spark.Services.ConsoleLoggerService">
            <summary>
            This logger service will be used if the .NET driver app did not configure a logger.
            Right now it just prints out the messages to Console
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Services.ConsoleLoggerService.IsDebugEnabled">
            <summary>
            Gets a value indicating whether logging is enabled for the Debug level.
            Always return true for the DefaultLoggerService object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.GetLoggerInstance(System.Type)">
            <summary>
            Get an instance of ILoggerService by a given type of logger
            </summary>
            <param name="type">The type of a logger to return</param>
            <returns>An instance of ILoggerService</returns>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogDebug(System.String)">
            <summary>
            Logs a message at debug level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogDebug(System.String,System.Object[])">
            <summary>
            Logs a message at debug level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogInfo(System.String)">
            <summary>
            Logs a message at info level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogInfo(System.String,System.Object[])">
            <summary>
            Logs a message at info level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogWarn(System.String)">
            <summary>
            Logs a message at warning level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogWarn(System.String,System.Object[])">
            <summary>
            Logs a message at warning level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogFatal(System.String)">
            <summary>
            Logs a fatal message.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogFatal(System.String,System.Object[])">
            <summary>
            Logs a fatal message with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogError(System.String)">
            <summary>
            Logs a error message.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogError(System.String,System.Object[])">
            <summary>
            Logs a error message with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogException(System.Exception)">
            <summary>
            Logs an exception
            </summary>
            <param name="e">The exception to be logged</param>
        </member>
        <member name="T:Microsoft.Spark.Services.IConfigurationService">
            <summary>
            Helps getting config settings to be used in .NET runtime
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.IConfigurationService.GetBackendPortNumber">
            <summary>
            The port number used for communicating with the .NET backend process.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.IConfigurationService.GetWorkerExePath">
            <summary>
            The full path to the .NET worker executable.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Services.ILoggerService">
            <summary>
            Defines a logger what be used in service
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Services.ILoggerService.IsDebugEnabled">
            <summary>
            Gets a value indicating whether logging is enabled for the Debug level.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.GetLoggerInstance(System.Type)">
            <summary>
            Get an instance of ILoggerService by a given type of logger
            </summary>
            <param name="type">The type of a logger to return</param>
            <returns>An instance of ILoggerService</returns>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogDebug(System.String)">
            <summary>
            Logs a message at debug level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogDebug(System.String,System.Object[])">
            <summary>
            Logs a message at debug level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogInfo(System.String)">
            <summary>
            Logs a message at info level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogInfo(System.String,System.Object[])">
            <summary>
            Logs a message at info level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogWarn(System.String)">
            <summary>
            Logs a message at warning level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogWarn(System.String,System.Object[])">
            <summary>
            Logs a message at warning level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogFatal(System.String)">
            <summary>
            Logs a fatal message.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogFatal(System.String,System.Object[])">
            <summary>
            Logs a fatal message with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogError(System.String)">
            <summary>
            Logs a error message.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogError(System.String,System.Object[])">
            <summary>
            Logs a error message with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogException(System.Exception)">
            <summary>
            Logs an exception
            </summary>
            <param name="e">The exception to be logged</param>
        </member>
        <member name="T:Microsoft.Spark.Services.LoggerServiceFactory">
            <summary>
            Used to get logger service instances for different types
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.LoggerServiceFactory.SetLoggerService(Microsoft.Spark.Services.ILoggerService)">
            <summary>
            Overrides an existing logger by a given logger service instance
            </summary>
            <param name="loggerServiceOverride">
            The logger service instance used to overrides
            </param>
        </member>
        <member name="M:Microsoft.Spark.Services.LoggerServiceFactory.GetLogger(System.Type)">
            <summary>
            Gets an instance of logger service for a given type.
            </summary>
            <param name="type">The type of logger service to get</param>
            <returns>An instance of logger service</returns>
        </member>
        <member name="M:Microsoft.Spark.Services.LoggerServiceFactory.GetDefaultLogger">
            <summary>
            if there exists xxx.exe.config file and log4net settings, then use log4net
            </summary>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.SparkConf">
            <summary>
            Configuration for a Spark application. Used to set various Spark parameters as key-value
            pairs.
            </summary>
            <remarks>
            Note that once a SparkConf object is passed to Spark, it is cloned and can no longer be
            modified by the user. Spark does not support modifying the configuration at runtime.
            </remarks>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.#ctor(System.Boolean)">
            <summary>
            Constructor.
            </summary>
            <param name="loadDefaults">
            Indicates whether to load values from Java system properties
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.SetMaster(System.String)">
            <summary>
            The master URL to connect to, such as "local" to run locally with one thread,
            "local[4]" to run locally with 4 cores, or "spark://master:7077" to run on a Spark
            standalone cluster.
            </summary>
            <param name="master">Spark master</param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.SetAppName(System.String)">
            <summary>
            Set a name for your application. Shown in the Spark web UI.
            </summary>
            <param name="appName">Name of the app</param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.SetSparkHome(System.String)">
            <summary>
            Set the location where Spark is installed on worker nodes.
            </summary>
            <param name="sparkHome"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.Set(System.String,System.String)">
            <summary>
            Set the value of a string config
            </summary>
            <param name="key">Config name</param>
            <param name="value">Config value</param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.GetInt(System.String,System.Int32)">
            <summary>
            Get a int parameter value, falling back to a default if not set
            </summary>
            <param name="key">Key to use</param>
            <param name="defaultValue">Default value to use</param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.Get(System.String,System.String)">
            <summary>
            Get a string parameter value, falling back to a default if not set
            </summary>
            <param name="key">Key to use</param>
            <param name="defaultValue">Default value to use</param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.GetAll">
            <summary>
            Get all parameters as a list of pairs.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.SparkContext">
            <summary>
            Main entry point for Spark functionality. A SparkContext represents the connection
            to a Spark cluster, and can be used to create RDDs, accumulators and broadcast
            variables on that cluster.
            
            Only one `SparkContext` should be active per JVM. You must `stop()` the
            active `SparkContext` before creating a new one.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor(Microsoft.Spark.SparkConf)">
            <summary>
            Create a SparkContext object with the given config.
            </summary>
            <param name="conf">a Spark config object describing the application configuration.
            Any settings in this config overrides the default configs as well as system properties.
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor">
            <summary>
            Create a SparkContext that loads settings from system properties (for instance,
            when launching with spark-submit).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor(System.String,System.String,Microsoft.Spark.SparkConf)">
            <summary>
            Alternative constructor that allows setting common Spark properties directly.
            </summary>
            <param name="master">Cluster URL to connect to (e.g. spark://host:port, local)</param>
            <param name="appName">A name for the application</param>
            <param name="conf">
            A <see cref="T:Microsoft.Spark.SparkConf"/> object specifying other Spark parameters
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor(System.String,System.String)">
            <summary>
            Initializes a SparkContext instance with a specific master and application name.
            </summary>
            <param name="master">Cluster URL to connect to (e.g. spark://host:port, local)</param>
            <param name="appName">A name for the application</param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor(System.String,System.String,System.String)">
            <summary>
            Alternative constructor that allows setting common Spark properties directly.
            </summary>
            <param name="master">Cluster URL to connect to (e.g. spark://host:port, local)</param>
            <param name="appName">A name for the application</param>
            <param name="sparkHome">The path that holds spark bits</param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Constructor where SparkContext object is already created.
            </summary>
            <param name="jvmObject">JVM object reference for this SparkContext object</param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.GetConf">
            <summary>
            Returns SparkConf object associated with this SparkContext object.
            Note that modifying the SparkConf object will not have any impact.
            </summary>
            <returns>SparkConf object</returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.GetOrCreate(Microsoft.Spark.SparkConf)">
            <summary>
            This function may be used to get or instantiate a SparkContext and register it as a
            singleton object. Because we can only have one active SparkContext per JVM,
            this is useful when applications may wish to share a SparkContext.
            </summary>
            <param name="conf"><see cref="T:Microsoft.Spark.SparkConf"/> that will be used for creating SparkContext
            </param>
            <returns>
            Current SparkContext (or a new one if it wasn't created before the function call)
            </returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.Stop">
            <summary>
            Shut down the SparkContext.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.SparkContext.DefaultParallelism">
            <summary>
            Default level of parallelism to use when not given by user (e.g. Parallelize()).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.GetUpdatedConf(System.String,System.String,System.String,Microsoft.Spark.SparkConf)">
            <summary>
            Creates a modified version of <see cref="T:Microsoft.Spark.SparkConf"/> with the parameters that can be
            passed separately to SparkContext, to make it easier to write SparkContext's
            constructors.
            </summary>
            <param name="master">Cluster URL to connect to (e.g. spark://host:port, local)</param>
            <param name="appName">A name for the application</param>
            <param name="sparkHome">The path that holds spark bits</param>
            <param name="conf">
            A <see cref="T:Microsoft.Spark.SparkConf"/> object specifying other Spark parameters
            </param>
            <returns>Modified <see cref="T:Microsoft.Spark.SparkConf"/> object.</returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.SetJobDescription(System.String)">
            <summary>
            Sets a human readable description of the current job.
            </summary>
            <param name="value">Description of the current job</param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.SetJobGroup(System.String,System.String,System.Boolean)">
            <summary>
            Assigns a group ID to all the jobs started by this thread until the group ID is set to
            a different value or cleared.
            </summary>
            <remarks>
            Often, a unit of execution in an application consists of multiple Spark actions or
            jobs. Application programmers can use this method to group all those jobs together
            and give a group description. Once set, the Spark web UI will associate such jobs
            with this group.
            </remarks>
            <param name="groupId">Group Id</param>
            <param name="description">Description on the job group</param>
            <param name="interruptOnCancel">
            If true, then job cancellation will result in `Thread.interrupt()` being called on the
            job's executor threads. 
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.ClearJobGroup">
            <summary>
            Clear the current thread's job group ID and its description.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.Parallelize``1(System.Collections.Generic.IEnumerable{``0},System.Nullable{System.Int32})">
            <summary>
            Distribute a local collection to form an RDD.
            </summary>
            <typeparam name="T">Type of the elements in the collection</typeparam>
            <param name="seq">Collection to distribute</param>
            <param name="numSlices">Number of partitions to divide the collection into</param>
            <returns>RDD representing distributed collection</returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.TextFile(System.String,System.Nullable{System.Int32})">
            <summary>
            Read a text file from HDFS, a local file system (available on all nodes), or any
            Hadoop-supported file system URI, and return it as an RDD of strings.
            </summary>
            <param name="path">path to the text file on a supported file system</param>
            <param name="minPartitions">minimum number of partitions for the resulting RDD</param>
            <returns>RDD of lines of the text file</returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.AddFile(System.String,System.Boolean)">
            <summary>
            Add a file to be downloaded with this Spark job on every node.
            </summary>
            <remarks>
            If a file is added during execution, it will not be available until the next
            TaskSet starts.
            </remarks>
            <param name="path">
            File path can be either a local file, a file in HDFS (or other Hadoop-supported
            filesystems), or an HTTP, HTTPS or FTP URI.
            </param>
            <param name="recursive">
            If true, a directory can be given in `path`. Currently directories are supported
            only for Hadoop-supported filesystems.
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.SetCheckpointDir(System.String)">
            <summary>
            Sets the directory under which RDDs are going to be checkpointed.
            </summary>
            <param name="directory">
            path to the directory where checkpoint files will be stored
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.WrapAsJavaRDD(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Returns JVM object reference to JavaRDD object transformed
            from a Scala RDD object.
            </summary>
            <remarks>
            The transformation is for easy reflection on the JVM side.
            </remarks>
            <param name="rdd">JVM object reference to Scala RDD</param>
            <returns>JVM object reference to JavaRDD object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowArrayHelpers">
            <summary>
            Helper methods to work with Apache Arrow arrays.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`1">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`2">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`3">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`4">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`5">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`6">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`7">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`8">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`9">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`10">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`11">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.Builder">
            <summary>
            The entry point to programming Spark with the Dataset and DataFrame API.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Master(System.String)">
            <summary>
            Sets the Spark master URL to connect to, such as "local" to run locally, "local[4]" to
            run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone
            cluster.
            </summary>
            <param name="master">Master URL</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.AppName(System.String)">
            <summary>
            Sets a name for the application, which will be shown in the Spark web UI.
            If no application name is set, a randomly generated name will be used.
            </summary>
            <param name="appName">Name of the app</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Config(System.String,System.String)">
            <summary>
            Sets a config option. Options set using this method are automatically propagated to
            both SparkConf and SparkSession's own configuration.
            </summary>
            <param name="key">Key for the configuration</param>
            <param name="value">value of the configuration</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Config(System.String,System.Boolean)">
            <summary>
            Sets a config option. Options set using this method are automatically propagated to
            both SparkConf and SparkSession's own configuration.
            </summary>
            <param name="key">Key for the configuration</param>
            <param name="value">value of the configuration</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Config(System.String,System.Double)">
            <summary>
            Sets a config option. Options set using this method are automatically propagated to
            both SparkConf and SparkSession's own configuration.
            </summary>
            <param name="key">Key for the configuration</param>
            <param name="value">value of the configuration</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Config(System.String,System.Int64)">
            <summary>
            Sets a config option. Options set using this method are automatically propagated to
            both SparkConf and SparkSession's own configuration.
            </summary>
            <param name="key">Key for the configuration</param>
            <param name="value">value of the configuration</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Config(Microsoft.Spark.SparkConf)">
            <summary>
            Sets a list of config options based on the given SparkConf
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.EnableHiveSupport">
            <summary>
            Enables Hive support, including connectivity to a persistent Hive metastore, support
            for Hive serdes, and Hive user-defined functions.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.GetOrCreate">
            <summary>
            Gets an existing [[SparkSession]] or, if there is no existing one, creates a new
            one based on the options set in this builder.
            </summary>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Column">
            <summary>
            Column class represents a column that will be computed based on the data in a DataFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Constructor for Column class.
            </summary>
            <param name="jvmObject">JVM object reference</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_UnaryNegation(Microsoft.Spark.Sql.Column)">
            <summary>
            Negate the given column.
            </summary>
            <param name="self">Column to negate</param>
            <returns>New column after applying negation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_LogicalNot(Microsoft.Spark.Sql.Column)">
            <summary>
            Apply inversion of boolean expression, i.e. NOT.
            </summary>
            <param name="self">Column to apply inversion</param>
            <returns>New column after applying inversion</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Equality(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply equality test on the given two columns.
            </summary>
            <param name="lhs">Column on the left side of equality test</param>
            <param name="rhs">Column on the right side of equality test</param>
            <returns>New column after applying equality test</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.EqualTo(System.Object)">
            <summary>
            Equality test.
            </summary>
            <param name="rhs">The right hand side of expression being tested for equality</param>
            <returns>New column after applying the equal to operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Inequality(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply inequality test.
            </summary>
            <param name="lhs">Column on the left side of inequality test</param>
            <param name="rhs">Column on the right side of inequality test</param>
            <returns>New column after applying inequality test</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.NotEqual(System.Object)">
            <summary>
            Inequality test.
            </summary>
            <param name="rhs">
            The right hand side of expression being tested for inequality.
            </param>
            <returns>New column after applying not equal operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_GreaterThan(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply "greater than" operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Gt(System.Object)">
            <summary>
            Greater than.
            </summary>
            <param name="rhs">
            The object that is in comparison to test if the left hand side is greater.
            </param>
            <returns>New column after applying the greater than operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_LessThan(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply "less than" operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Lt(System.Object)">
            <summary>
            Less than.
            </summary>
            <param name="rhs">
            The object that is in comparison to test if the left hand side is lesser.
            </param>
            <returns>New column after applying the less than operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_LessThanOrEqual(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply "less than or equal to" operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Leq(System.Object)">
            <summary>
            Less than or equal to.
            </summary>
            <param name="rhs">
            The object that is in comparison to test if the left hand side is less or equal to.
            </param>
            <returns>New column after applying the less than or equal to operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_GreaterThanOrEqual(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply "greater than or equal to" operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Geq(System.Object)">
            <summary>
            Greater or equal to.
            </summary>
            <param name="rhs">
            The object that is in comparison to test if the left hand side is greater or equal to
            </param>
            <returns>New column after applying the greater or equal to operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.EqNullSafe(System.Object)">
            <summary>
            Apply equality test that is safe for null values.
            </summary>
            <param name="obj">Object to apply equality test</param>
            <returns>New column after applying the equality test</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.When(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Evaluates a condition and returns one of multiple possible result expressions.
            If Otherwise(object) is not defined at the end, null is returned for
            unmatched conditions. This method can be chained with other 'when' invocations in case
            multiple matches are required.
            </summary>
            <param name="condition">The condition to check</param>
            <param name="value">The value to set if the condition is true</param>
            <returns>New column after applying the when method</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Otherwise(System.Object)">
            <summary>
            Evaluates a list of conditions and returns one of multiple possible result expressions.
            If otherwise is not defined at the end, null is returned for unmatched conditions.
            This is used when the When(Column, object) method is applied.
            </summary>
            <param name="value">The value to set</param>
            <returns>New column after applying otherwise method</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Between(System.Object,System.Object)">
            <summary>
            True if the current column is between the lower bound and upper bound, inclusive.
            </summary>
            <param name="lowerBound">The lower bound</param>
            <param name="upperBound">The upper bound</param>
            <returns>New column after applying the between method</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsNaN">
            <summary>
            True if the current expression is NaN.
            </summary>
            <returns>
            New column with values true if the preceding column had a NaN
            value in the same index, and false otherwise.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsNull">
            <summary>
            True if the current expression is null.
            </summary>
            <returns>
            New column with values true if the preceding column had a null
            value in the same index, and false otherwise.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsNotNull">
            <summary>
            True if the current expression is NOT null.
            </summary>
            <returns>
            New column with values true if the preceding column had a non-null
            value in the same index, and false otherwise.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_BitwiseOr(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Apply boolean OR operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Or(Microsoft.Spark.Sql.Column)">
            <summary>
            Apply boolean OR operator with the given column.
            </summary>
            <param name="other">Column to apply OR operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_BitwiseAnd(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Apply boolean AND operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.And(Microsoft.Spark.Sql.Column)">
            <summary>
            Apply boolean AND operator with the given column.
            </summary>
            <param name="other">Column to apply AND operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Addition(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply sum of two expressions.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Object on the right side of the operator</param>
            <returns>New column after applying the sum operation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Plus(System.Object)">
            <summary>
            Sum of this expression and another expression.
            </summary>
            <param name="rhs">The expression to be summed with</param>
            <returns>New column after applying the plus operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Subtraction(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply subtraction of two expressions.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Object on the right side of the operator</param>
            <returns>New column after applying the subtraction operation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Minus(System.Object)">
            <summary>
            Subtraction. Subtract the other expression from this expression.
            </summary>
            <param name="rhs">The expression to be subtracted with</param>
            <returns>New column after applying the minus operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Multiply(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply multiplication of two expressions.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Object on the right side of the operator</param>
            <returns>New column after applying the multiplication operation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Multiply(System.Object)">
            <summary>
            Multiplication of this expression and another expression.
            </summary>
            <param name="rhs">The expression to be multiplied with</param>
            <returns>New column after applying the multiply operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Division(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply division of two expressions.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Object on the right side of the operator</param>
            <returns>New column after applying the division operation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Divide(System.Object)">
            <summary>
            Division of this expression by another expression.
            </summary>
            <param name="rhs">The expression to be divided by</param>
            <returns>New column after applying the divide operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Modulus(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply division of two expressions.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Object on the right side of the operator</param>
            <returns>New column after applying the division operation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Mod(System.Object)">
            <summary>
            Modulo (a.k.a remainder) expression.
            </summary>
            <param name="rhs">
            The expression to be divided by to get the remainder for.
            </param>
            <returns>New column after applying the mod operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Like(System.String)">
            <summary>
            SQL like expression. Returns a boolean column based on a SQL LIKE match.
            </summary>
            <param name="literal">The literal that is used to compute the SQL LIKE match</param>
            <returns>New column after applying the SQL LIKE match</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.RLike(System.String)">
            <summary>
            SQL RLIKE expression (LIKE with Regex). Returns a boolean column based on a regex
            match.
            </summary>
            <param name="literal">The literal that is used to compute the Regex match</param>
            <returns>New column after applying the regex LIKE method</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.GetItem(System.Object)">
            <summary>
            An expression that gets an item at position `ordinal` out of an array,
            or gets a value by key `key` in a `MapType`.
            </summary>
            <param name="key">The key with which to identify the item</param>
            <returns>New column after getting an item given a specific key</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.GetField(System.String)">
            <summary>
            An expression that gets a field by name in a `StructType`.
            </summary>
            <param name="fieldName">The name of the field</param>
            <returns>New column after getting a field for a specific key</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.SubStr(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            An expression that returns a substring.
            </summary>
            <param name="startPos">Expression for the starting position</param>
            <param name="len">Expression for the length of the substring</param>
            <returns>
            New column that is bound by the start position provided, and the length.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.SubStr(System.Int32,System.Int32)">
            <summary>
            An expression that returns a substring.
            </summary>
            <param name="startPos">Starting position</param>
            <param name="len">Length of the substring</param>
            <returns>
            New column that is bound by the start position provided, and the length.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Contains(System.Object)">
            <summary>
            Contains the other element. Returns a boolean column based on a string match.
            </summary>
            <param name="other">
            The object that is used to check for existance in the current column.
            </param>
            <returns>New column after checking if the column contains object other</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.StartsWith(Microsoft.Spark.Sql.Column)">
            <summary>
            String starts with. Returns a boolean column based on a string match.
            </summary>
            <param name="other">
            The other column containing strings with which to check how values
            in this column starts.
            </param>
            <returns>
            A boolean column where entries are true if values in the current
            column does indeed start with the values in the given column.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.StartsWith(System.String)">
            <summary>
            String starts with another string literal.
            Returns a boolean column based on a string match.
            </summary>
            <param name="literal">
            The string literal used to check how values in a column starts.
            </param>
            <returns>
            A boolean column where entries are true if values in the current column
            does indeed start with the given string literal.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.EndsWith(Microsoft.Spark.Sql.Column)">
            <summary>
            String ends with. Returns a boolean column based on a string match.
            </summary>
            <param name="other">
            The other column containing strings with which to check how values
            in this column ends.
            </param>
            <returns>
            A boolean column where entries are true if values in the current
            column does indeed end with the values in the given column.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.EndsWith(System.String)">
            <summary>
            String ends with another string literal. Returns a boolean column based
            on a string match.
            </summary>
            <param name="literal">
            The string literal used to check how values in a column ends.
            </param>
            <returns>
            A boolean column where entries are true if values in the current column
            does indeed end with the given string literal.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Alias(System.String)">
            <summary>
            Gives the column an alias. Same as `As()`.
            </summary>
            <param name="alias">The alias that is given</param>
            <returns>New column after applying an alias</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.As(System.String)">
            <summary>
            Gives the column an alias.
            </summary>
            <param name="alias">The alias that is given</param>
            <returns>New column after applying the as alias operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.As(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Assigns the given aliases to the results of a table generating function.
            </summary>
            <param name="alias">A list of aliases</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Name(System.String)">
            <summary>
            Gives the column a name (alias).
            </summary>
            <param name="alias">Alias column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Cast(System.String)">
            <summary>
            Casts the column to a different data type, using the canonical string
            representation of the type.
            </summary>
            <remarks>
            The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
            `float`, `double`, `decimal`, `date`, `timestamp`.
            </remarks>
            <param name="to">String version of datatype</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Desc">
            <summary>
            Returns a sort expression based on ascending order of the column,
            and null values return before non-null values.
            </summary>
            <returns>New column after applying the descending order operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.DescNullsFirst">
            <summary>
            Returns a sort expression based on the descending order of the column,
            and null values appear before non-null values.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.DescNullsLast">
            <summary>
            Returns a sort expression based on the descending order of the column,
            and null values appear after non-null values.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Asc">
            <summary>
            Returns a sort expression based on ascending order of the column.
            </summary>
            <returns>New column after applying the ascending order operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.AscNullsFirst">
            <summary>
            Returns a sort expression based on ascending order of the column,
            and null values return before non-null values.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.AscNullsLast">
            <summary>
            Returns a sort expression based on ascending order of the column,
            and null values appear after non-null values.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Explain(System.Boolean)">
            <summary>
            Prints the expression to the console for debugging purposes.
            </summary>
            <param name="extended">To print extended version or not</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.BitwiseOR(System.Object)">
            <summary>
            Compute bitwise OR of this expression with another expression.
            </summary>
            <param name="other">
            The other column that will be used to compute the bitwise OR.
            </param>
            <returns>New column after applying bitwise OR operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.BitwiseAND(System.Object)">
            <summary>
            Compute bitwise AND of this expression with another expression.
            </summary>
            <param name="other">
            The other column that will be used to compute the bitwise AND.
            </param>
            <returns>New column after applying the bitwise AND operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.BitwiseXOR(System.Object)">
            <summary>
            Compute bitwise XOR of this expression with another expression.
            </summary>
            <param name="other">
            The other column that will be used to compute the bitwise XOR.
            </param>
            <returns>New column after applying bitwise XOR operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Over(Microsoft.Spark.Sql.Expressions.WindowSpec)">
            <summary>
            Defines a windowing column.
            </summary>
            <param name="window">
            A window specification that defines the partitioning, ordering, and frame boundaries.
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Over">
            <summary>
            Defines an empty analytic clause. In this case the analytic function is applied
            and presented for all rows in the result set.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Equals(System.Object)">
            <summary>
            Checks if the given object is equal to this object.
            </summary>
            <param name="obj">Object to compare to</param>
            <returns>True if the given object is equal to this object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.GetHashCode">
            <summary>
            Calculates the hash code for this object.
            </summary>
            <returns>Hash code for this object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.ApplyFunction(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Invokes a method under "org.apache.spark.sql.functions" with the given column.
            </summary>
            <param name="column">Column to apply function</param>
            <param name="name">Name of the function</param>
            <returns>New column after applying the function</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Apply(System.String)">
            <summary>
            Invokes an operator (method name) with the current column.
            </summary>
            <param name="op">Operator to invoke</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Apply(System.String,System.Object)">
            <summary>
            Invokes an operator (method name) with the current column with other object.
            </summary>
            <param name="op">Operator to invoke</param>
            <param name="other">Object to apply the operator with</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Apply(System.String,System.Object,System.Object)">
            <summary>
            Invokes a method name with the current column with two other objects as parameters.
            </summary>
            <param name="op">Method to invoke</param>
            <param name="other1">Object to apply the method with</param>
            <param name="other2">Object to apply the method with</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrame">
            TODO:
            Missing APIs:
            Persist() with "StorageLevel"
            <summary>
             A distributed collection of data organized into named columns.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.DataFrame.Item(System.String)">
            <summary>
            Selects column based on the column name.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.ToDF">
            <summary>
            Converts this strongly typed collection of data to generic `DataFrame`.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.ToDF(System.String[])">
            <summary>
            Converts this strongly typed collection of data to generic `DataFrame`
            with columns renamed.
            </summary>
            <param name="colNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Schema">
            <summary>
            Returns the schema associated with this `DataFrame`.
            </summary>
            <returns>Schema associated with this data frame</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.PrintSchema">
            <summary>
            Prints the schema to the console in a nice tree format.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Explain(System.Boolean)">
            <summary>
            Prints the plans (logical and physical) to the console for debugging purposes.
            </summary>
            <param name="extended">prints only physical if set to false</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Columns">
            <summary>
            Returns all column names.
            </summary>
            <returns>Column names</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.IsLocal">
            <summary>
            Returns true if the Collect() and Take() methods can be run locally without any
            Spark executors.
            </summary>
            <returns>True if Collect() and Take() can be run locally</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.IsEmpty">
            <summary>
            Returns true if this DataFrame is empty.
            </summary>
            <returns>True if empty</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.IsStreaming">
            <summary>
            Returns true if this `DataFrame` contains one or more sources that continuously
            return data as it arrives.
            </summary>
            <returns>True if streaming DataFrame</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Checkpoint(System.Boolean)">
            <summary>
            Returns a checkpointed version of this `DataFrame`.
            </summary>
            <remarks>
            Checkpointing can be used to truncate the logical plan of this `DataFrame`, which is
            especially useful in iterative algorithms where the plan may grow exponentially.
            It will be saved to files inside the checkpoint directory set with
            <see cref="M:Microsoft.Spark.SparkContext.SetCheckpointDir(System.String)"/>.
            </remarks>
            <param name="eager">Whether to checkpoint this `DataFrame` immediately</param>
            <returns>Checkpointed DataFrame</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.LocalCheckpoint(System.Boolean)">
            <summary>
            Returns a locally checkpointed version of this `DataFrame`.
            </summary>
            <remarks>
            Checkpointing can be used to truncate the logical plan of this `DataFrame`, which is
            especially useful in iterative algorithms where the plan may grow exponentially.
            Local checkpoints are written to executor storage and despite potentially faster
            they are unreliable and may compromise job completion.
            </remarks>
            <param name="eager">Whether to checkpoint this `DataFrame` immediately</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.WithWatermark(System.String,System.String)">
            <summary>
            Defines an event time watermark for this DataFrame. A watermark tracks a point in time
            before which we assume no more late data is going to arrive.
            </summary>
            <param name="eventTime">
            The name of the column that contains the event time of the row.
            </param>
            <param name="delayThreshold">
            The minimum delay to wait to data to arrive late, relative to the latest record that
            has been processed in the form of an interval (e.g. "1 minute" or "5 hours").
            </param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Show(System.Int32,System.Int32,System.Boolean)">
            <summary>
            Displays rows of the `DataFrame` in tabular form.
            </summary>
            <param name="numRows">Number of rows to show</param>
            <param name="truncate">If set to more than 0, truncates strings to `truncate`
                                   characters and all cells will be aligned right.</param>
            <param name="vertical">If set to true, prints output rows vertically
                                   (one line per column value).</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Na">
            <summary>
            Returns a `DataFrameNaFunctions` for working with missing data.
            </summary>
            <returns>DataFrameNaFunctions object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Stat">
             <summary>
            Returns a `DataFrameStatFunctions` for working statistic functions support.
             </summary>
             <returns>DataFrameNaFunctions object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Join(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Join with another `DataFrame`.
            </summary>
            <remarks>
            Behaves as an INNER JOIN and requires a subsequent join predicate.
            </remarks>
            <param name="right">Right side of the join operator</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Join(Microsoft.Spark.Sql.DataFrame,System.String)">
            <summary>
            Inner equi-join with another `DataFrame` using the given column.
            </summary>
            <param name="right">Right side of the join operator</param>
            <param name="usingColumn">
            Name of the column to join on. This column must exist on both sides.
            </param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Join(Microsoft.Spark.Sql.DataFrame,System.Collections.Generic.IEnumerable{System.String},System.String)">
            <summary>
            Equi-join with another `DataFrame` using the given columns. A cross join with
            a predicate is specified as an inner join. If you would explicitly like to
            perform a cross join use the `crossJoin` method.
            </summary>
            <param name="right">Right side of the join operator</param>
            <param name="usingColumns">Name of columns to join on</param>
            <param name="joinType">Type of join to perform. Default `inner`. Must be one of:
            `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`,
            `right_outer`, `left_semi`, `left_anti`</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Join(Microsoft.Spark.Sql.DataFrame,Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Join with another `DataFrame`, using the given join expression.
            </summary>
            <param name="right">Right side of the join operator</param>
            <param name="joinExpr">Join expression</param>
            <param name="joinType">Type of join to perform. Default `inner`. Must be one of:
            `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`,
            `right_outer`, `left_semi`, `left_anti`.</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.CrossJoin(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Explicit Cartesian join with another `DataFrame`.
            </summary>
            <remarks>
            Cartesian joins are very expensive without an extra filter that can be pushed down.
            </remarks>
            <param name="right">Right side of the join operator</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.SortWithinPartitions(System.String,System.String[])">
            <summary>
            Returns a new `DataFrame` with each partition sorted by the given expressions.
            </summary>
            <remarks>
            This is the same operation as "SORT BY" in SQL (Hive QL).
            </remarks>
            <param name="column">Column name to sort by</param>
            <param name="columns">Additional column names to sort by</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.SortWithinPartitions(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` with each partition sorted by the given expressions.
            </summary>
            <remarks>
            This is the same operation as "SORT BY" in SQL (Hive QL).
            </remarks>
            <param name="columns">Column expressions to sort by</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Sort(System.String,System.String[])">
            <summary>
            Returns a new `DataFrame` sorted by the specified column, all in ascending order.
            </summary>
            <param name="column">Column name to sort by</param>
            <param name="columns">Additional column names to sort by</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Sort(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` sorted by the given expressions.
            </summary>
            <param name="columns">Column expressions to sort by</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.OrderBy(System.String,System.String[])">
            <summary>
            Returns a new Dataset sorted by the given expressions.
            </summary>
            <remarks>
            This is an alias of the Sort() function.
            </remarks>
            <param name="column">Column name to sort by</param>
            <param name="columns">Additional column names to sort by</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.OrderBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new Dataset sorted by the given expressions.
            </summary>
            <remarks>
            This is an alias of the Sort() function.
            </remarks>
            <param name="columns">Column expressions to sort by</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Hint(System.String,System.Object[])">
            <summary>
            Specifies some hint on the current `DataFrame`.
            </summary>
            <remarks>
            Due to the limitation of the type conversion between CLR and JVM,
            the type of object in `parameters` should be the same.
            </remarks>
            <param name="name">Name of the hint</param>
            <param name="parameters">Parameters of the hint</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Col(System.String)">
            <summary>
            Selects column based on the column name.
            </summary>
            <param name="colName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.ColRegex(System.String)">
            <summary>
            Selects column based on the column name specified as a regex.
            </summary>
            <param name="colName">Column name as a regex</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.As(System.String)">
            <summary>
            Returns a new `DataFrame` with an alias set.
            </summary>
            <param name="alias">Alias name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Alias(System.String)">
            <summary>
            Returns a new `DataFrame` with an alias set. Same as As().
            </summary>
            <param name="alias">Alias name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Select(Microsoft.Spark.Sql.Column[])">
            <summary>
            Selects a set of column based expressions.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Select(System.String,System.String[])">
            <summary>
            Selects a set of columns. This is a variant of Select() that can only select
            existing columns using column names (i.e. cannot construct expressions).
            </summary>
            <param name="column">Column name</param>
            <param name="columns">Additional column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.SelectExpr(System.String[])">
            <summary>
            Selects a set of SQL expressions. This is a variant of Select() that
            accepts SQL expressions.
            </summary>
            <param name="expressions"></param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Filter(Microsoft.Spark.Sql.Column)">
            <summary>
            Filters rows using the given condition.
            </summary>
            <param name="condition">Condition expression</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Filter(System.String)">
            <summary>
            Filters rows using the given SQL expression.
            </summary>
            <param name="conditionExpr">SQL expression</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Where(Microsoft.Spark.Sql.Column)">
            <summary>
            Filters rows using the given condition. This is an alias for Filter().
            </summary>
            <param name="condition">Condition expression</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Where(System.String)">
            <summary>
            Filters rows using the given SQL expression. This is an alias for Filter().
            </summary>
            <param name="conditionExpr">SQL expression</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.GroupBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Groups the DataFrame using the specified columns, so we can run aggregation on them.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.GroupBy(System.String,System.String[])">
            <summary>
            Groups the DataFrame using the specified columns.
            </summary>
            <param name="column">Column name</param>
            <param name="columns">Additional column names</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Rollup(Microsoft.Spark.Sql.Column[])">
            <summary>
            Create a multi-dimensional rollup for the current `DataFrame` using the
            specified columns.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Rollup(System.String,System.String[])">
            <summary>
            Create a multi-dimensional rollup for the current `DataFrame` using the
            specified columns.
            </summary>
            <param name="column">Column name</param>
            <param name="columns">Additional column names</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Cube(Microsoft.Spark.Sql.Column[])">
            <summary>
            Create a multi-dimensional cube for the current `DataFrame` using the
            specified columns.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Cube(System.String,System.String[])">
            <summary>
            Create a multi-dimensional cube for the current `DataFrame` using the
            specified columns.
            </summary>
            <param name="column">Column name</param>
            <param name="columns">Additional column names</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Agg(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column[])">
            <summary>
            Aggregates on the entire `DataFrame` without groups.
            </summary>
            <param name="expr">Column expression to aggregate</param>
            <param name="exprs">Additional column expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Limit(System.Int32)">
            <summary>
            Returns a new `DataFrame` by taking the first `number` rows.
            </summary>
            <param name="n">Number of rows to take</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Union(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing union of rows in this `DataFrame`
            and another `DataFrame`.
            </summary>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.UnionByName(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing union of rows in this `DataFrame`
            and another `DataFrame`, resolving columns by name.
            </summary>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Intersect(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing rows only in both this `DataFrame`
            and another `DataFrame`.
            </summary>
            <remarks>
            This is equivalent to `INTERSECT` in SQL.
            </remarks>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.IntersectAll(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing rows only in both this `DataFrame`
            and another `DataFrame` while preserving the duplicates.
            </summary>
            <remarks>
            This is equivalent to `INTERSECT ALL` in SQL.
            </remarks>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Except(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing rows in this `DataFrame` but
            not in another `DataFrame`.
            </summary>
            <remarks>
            This is equivalent to `EXCEPT DISTINCT` in SQL.
            </remarks>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.ExceptAll(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing rows in this `DataFrame` but
            not in another `DataFrame` while preserving the duplicates.
            </summary>
            <remarks>
            This is equivalent to `EXCEPT ALL` in SQL.
            </remarks>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Sample(System.Double,System.Boolean,System.Nullable{System.Int64})">
            <summary>
            Returns a new `DataFrame` by sampling a fraction of rows (without replacement),
            using a user-supplied seed.
            </summary>
            <param name="fraction">Fraction of rows</param>
            <param name="withReplacement">Sample with replacement or not</param>
            <param name="seed">Optional random seed</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.RandomSplit(System.Double[],System.Nullable{System.Int64})">
            <summary>
            Randomly splits this `DataFrame` with the provided weights.
            </summary>
            <param name="weights">Weights for splits</param>
            <param name="seed">Optional random seed</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.WithColumn(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a new `DataFrame` by adding a column or replacing the existing column that
            has the same name.
            </summary>
            <param name="colName">Name of the new column</param>
            <param name="col">Column expression for the new column</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.WithColumnRenamed(System.String,System.String)">
            <summary>
            Returns a new Dataset with a column renamed.
            This is a no-op if schema doesn't contain `existingName`.
            </summary>
            <param name="existingName">Existing column name</param>
            <param name="newName">New column name to replace with</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Drop(System.String[])">
            <summary>
            Returns a new `DataFrame` with columns dropped.
            This is a no-op if schema doesn't contain column name(s).
            </summary>
            <param name="colNames">Name of columns to drop</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Drop(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a new `DataFrame` with a column dropped.
            This is a no-op if the `DataFrame` doesn't have a column with an equivalent expression.
            </summary>
            <param name="col">Column expression</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.DropDuplicates">
            <summary>
            Returns a new `DataFrame` that contains only the unique rows from this `DataFrame`.
            This is an alias for Distinct().
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.DropDuplicates(System.String,System.String[])">
            <summary>
            Returns a new `DataFrame` with duplicate rows removed, considering only
            the subset of columns.
            </summary>
            <param name="col">Column name</param>
            <param name="cols">Additional column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Describe(System.String[])">
            <summary>
            Computes basic statistics for numeric and string columns, including count, mean,
            stddev, min, and max. If no columns are given, this function computes statistics for
            all numerical or string columns.
            </summary>
            <remarks>
            This function is meant for exploratory data analysis, as we make no guarantee about
            the backward compatibility of the schema of the resulting DataFrame. If you want to
            programmatically compute summary statistics, use the `agg` function instead.
            </remarks>
            <param name="cols">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Summary(System.String[])">
             <summary>
             Computes specified statistics for numeric and string columns.
             </summary>
             <remarks>
             Available statistics are:
             - count
             - mean
             - stddev
             - min
             - max
             - arbitrary approximate percentiles specified as a percentage(e.g., 75%)
            
             If no statistics are given, this function computes count, mean, stddev, min,
             approximate quartiles(percentiles at 25%, 50%, and 75%), and max.
             </remarks>
             <param name="statistics">Statistics to compute</param>
             <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Head(System.Int32)">
            <summary>
            Returns the first `n` rows.
            </summary>
            <param name="n">Number of rows</param>
            <returns>First `n` rows</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Head">
            <summary>
            Returns the first row.
            </summary>
            <returns>First row</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.First">
            <summary>
            Returns the first row. Alis for Head().
            </summary>
            <returns>First row</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Take(System.Int32)">
            <summary>
            Returns the first `n` rows in the `DataFrame`.
            </summary>
            <param name="n">Number of rows</param>
            <returns>First `n` rows</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Collect">
            <summary>
            Returns an array that contains all rows in this `DataFrame`.
            </summary>
            <remarks>
            This requires moving all the data into the application's driver process, and
            doing so on a very large dataset can crash the driver process with OutOfMemoryError.
            </remarks>
            <returns>Row objects</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.ToLocalIterator">
            <summary>
            Returns an iterator that contains all of the rows in this `DataFrame`.
            The iterator will consume as much memory as the largest partition in this `DataFrame`.
            </summary>
            <returns>Row objects</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Count">
            <summary>
            Returns the number of rows in the `DataFrame`.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Repartition(System.Int32)">
            <summary>
            Returns a new `DataFrame` that has exactly `numPartitions` partitions.
            </summary>
            <param name="numPartitions">Number of partitions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Repartition(System.Int32,Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` partitioned by the given partitioning expressions into
            `numPartitions`. The resulting `DataFrame` is hash partitioned.
            </summary>
            <param name="numPartitions">Number of partitions</param>
            <param name="partitionExprs">Partitioning expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Repartition(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` partitioned by the given partitioning expressions, using
            `spark.sql.shuffle.partitions` as number of partitions.
            </summary>
            <param name="partitionExprs">Partitioning expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.RepartitionByRange(System.Int32,Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` partitioned by the given partitioning expressions into
            `numPartitions`. The resulting `DataFrame` is range partitioned.
            </summary>
            <param name="numPartitions">Number of partitions</param>
            <param name="partitionExprs">Partitioning expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.RepartitionByRange(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` partitioned by the given partitioning expressions, using
            `spark.sql.shuffle.partitions` as number of partitions.
            The resulting Dataset is range partitioned.
            </summary>
            <param name="partitionExprs">Partitioning expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Coalesce(System.Int32)">
            <summary>
            Returns a new `DataFrame` that has exactly `numPartitions` partitions, when the
            fewer partitions are requested. If a larger number of partitions is requested,
            it will stay at the current number of partitions.
            </summary>
            <param name="numPartitions">Number of partitions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Distinct">
            <summary>
            Returns a new Dataset that contains only the unique rows from this `DataFrame`.
            This is an alias for DropDuplicates().
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Persist">
            <summary>
            Persist this `DataFrame` with the default storage level (`MEMORY_AND_DISK`).
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Cache">
            <summary>
            Persist this `DataFrame` with the default storage level (`MEMORY_AND_DISK`).
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Unpersist(System.Boolean)">
            <summary>
            Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
            </summary>
            <remarks>
            This will not un-persist any cached data that is built upon this `DataFrame`.
            </remarks>
            <param name="blocking"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.CreateTempView(System.String)">
            <summary>
            Creates a local temporary view using the given name. The lifetime of this
            temporary view is tied to the SparkSession that created this `DataFrame`.
            </summary>
            <param name="viewName">Name of the view</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.CreateOrReplaceTempView(System.String)">
            <summary>
            Creates or replaces a local temporary view using the given name. The lifetime of this
            temporary view is tied to the SparkSession that created this `DataFrame`.
            </summary>
            <param name="viewName">Name of the view</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.CreateGlobalTempView(System.String)">
            <summary>
            Creates a global temporary view using the given name. The lifetime of this
            temporary view is tied to this Spark application.
            </summary>
            <param name="viewName">Name of the view</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.CreateOrReplaceGlobalTempView(System.String)">
            <summary>
            Creates or replaces a global temporary view using the given name. The lifetime of this
            temporary view is tied to this Spark application.
            </summary>
            <param name="viewName">Name of the view</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Write">
            <summary>
            Interface for saving the content of the non-streaming Dataset out
            into external storage.
            </summary>
            <returns>DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.WriteStream">
            <summary>
            Interface for saving the content of the streaming Dataset out into external storage.
            </summary>
            <returns>DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.GetRows(System.String)">
            <summary>
            Returns row objects based on the function (either "toPythonIterator" or
            "collectToPython").
            </summary>
            <param name="funcName"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.GetConnectionInfo(System.String)">
            <summary>
            Returns a tuple of port number and secret string which are
            used for connecting with Spark to receive rows for this `DataFrame`.
            </summary>
            <returns>A tuple of port number and secret string</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameNaFunctions">
            <summary>
            Provides functionalities for working with missing data in <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop">
            <summary>
            Returns a new `DataFrame` that drops rows containing any null or NaN values.
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop(System.String)">
            <summary>
            Returns a new `DataFrame` that drops rows containing null or NaN values.
            </summary>
            <remarks>
            If `how` is "any", then drop rows containing any null or NaN values.
            If `how` is "all", then drop rows only if every column is null or NaN for that row.
            </remarks>
            <param name="how">Determines the behavior of dropping rows</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that drops rows containing any null or NaN values
            in the specified columns.
            </summary>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop(System.String,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that drops rows containing any null or NaN values
            in the specified columns.
            </summary>
            <remarks>
            If `how` is "any", then drop rows containing any null or NaN values.
            If `how` is "all", then drop rows only if every column is null or NaN for that row.
            </remarks>
            <param name="how">Determines the behavior of dropping rows</param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop(System.Int32)">
            <summary>
            Returns a new `DataFrame` that drops rows containing less than `minNonNulls`
            non-null and non-NaN values.
            </summary>
            <param name="minNonNulls"></param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop(System.Int32,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that drops rows containing less than `minNonNulls`
            non-null and non-NaN values in the specified columns.
            </summary>
            <param name="minNonNulls"></param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Int64)">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in numeric columns
            with `value`.
            </summary>
            <param name="value">Value to replace with</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Double)">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in numeric columns
            with `value`.
            </summary>
            <param name="value">Value to replace with</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.String)">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in numeric columns
            with `value`.
            </summary>
            <param name="value">Value to replace with</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Int64,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in specified numeric
            columns. If a specified column is not a numeric column, it is ignored.
            </summary>
            <param name="value">Value to replace with</param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Double,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in specified numeric
            columns. If a specified column is not a numeric column, it is ignored.
            </summary>
            <param name="value">Value to replace with</param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.String,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in specified string
            columns. If a specified column is not a string column, it is ignored.
            </summary>
            <param name="value">Value to replace with</param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Boolean)">
            <summary>
            Returns a new `DataFrame` that replaces null values in boolean columns with `value`.
            </summary>
            <param name="value">Value to replace with</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Boolean,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in specified boolean
            columns. If a specified column is not a boolean column, it is ignored.
            </summary>
            <param name="value">Value to replace with</param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.IDictionary{System.String,System.Int32})">
            <summary>
            Returns a new `DataFrame` that replaces null values.
            </summary>
            <remarks>
            The key of the map is the column name, and the value of the map is the
            replacement value.
            </remarks>
            <param name="valueMap">Values to replace null values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.IDictionary{System.String,System.Int64})">
            <summary>
            Returns a new `DataFrame` that replaces null values.
            </summary>
            <remarks>
            The key of the map is the column name, and the value of the map is the
            replacement value.
            </remarks>
            <param name="valueMap">Values to replace null values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.IDictionary{System.String,System.Double})">
            <summary>
            Returns a new `DataFrame` that replaces null values.
            </summary>
            <remarks>
            The key of the map is the column name, and the value of the map is the
            replacement value.
            </remarks>
            <param name="valueMap">Values to replace null values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Returns a new `DataFrame` that replaces null values.
            </summary>
            <remarks>
            The key of the map is the column name, and the value of the map is the
            replacement value.
            </remarks>
            <param name="valueMap">Values to replace null values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.IDictionary{System.String,System.Boolean})">
            <summary>
            Returns a new `DataFrame` that replaces null values.
            </summary>
            <remarks>
            The key of the map is the column name, and the value of the map is the
            replacement value.
            </remarks>
            <param name="valueMap">Values to replace null values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.String,System.Collections.Generic.IDictionary{System.Double,System.Double})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnName">
            Name of the column to apply the value replacement. If `col` is "*", replacement
            is applied on all string, numeric or boolean columns.
            </param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.String,System.Collections.Generic.IDictionary{System.Boolean,System.Boolean})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnName">
            Name of the column to apply the value replacement. If `col` is "*", replacement
            is applied on all string, numeric or boolean columns.
            </param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.String,System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnName">
            Name of the column to apply the value replacement. If `col` is "*", replacement
            is applied on all string, numeric or boolean columns.
            </param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.Collections.Generic.IEnumerable{System.String},System.Collections.Generic.IDictionary{System.Double,System.Double})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnNames">
            Name of the column to apply the value replacement. If `col` is "*", replacement
            is applied on all string, numeric or boolean columns.
            </param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.Collections.Generic.IEnumerable{System.String},System.Collections.Generic.IDictionary{System.Boolean,System.Boolean})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnNames">list of columns to apply the value replacement.</param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.Collections.Generic.IEnumerable{System.String},System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnNames">list of columns to apply the value replacement.</param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameReader">
            <summary>
            DataFrameReader provides functionality to load a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>
            from external storage systems (e.g. file systems, key-value stores, etc).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Format(System.String)">
            <summary>
            Specifies the input data source format.
            </summary>
            <param name="source">Name of the data source</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Schema(System.String)">
            <summary>
            Specifies the schema by using the given DDL-formatted string.
            </summary>
            <remarks>
            Some data sources (e.g. JSON) can infer the input schema automatically
            from data. By specifying the schema here, the underlying data source can
            skip the schema inference step, and thus speed up data loading.
            </remarks>
            <param name="schemaString">DDL-formatted string</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Option(System.String,System.String)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Option(System.String,System.Boolean)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Option(System.String,System.Int64)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Option(System.String,System.Double)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Options(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Adds input options for the underlying data source.
            </summary>
            <param name="options">Key/value options</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Load(System.String[])">
            <summary>
            Loads input in as a DataFrame from the given paths.
            </summary>
            <remarks>
            Paths can be empty if data sources don't require a path (e.g. external
            key-value stores).
            </remarks>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Json(System.String[])">
            <summary>
            Loads a JSON file (one object per line) and returns the result as a DataFrame.
            </summary>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Csv(System.String[])">
            <summary>
            Loads CSV files and returns the result as a DataFrame.
            </summary>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Parquet(System.String[])">
            <summary>
            Loads a Parquet file, returning the result as a DataFrame.
            </summary>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Orc(System.String[])">
            <summary>
            Loads an ORC file and returns the result as a DataFrame.
            </summary>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Text(System.String[])">
            <summary>
            Loads text files and returns a DataFrame whose schema starts with a string column
            named "value", and followed by partitioned columns if there are any.
            </summary>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.OptionInternal(System.String,System.Object)">
            <summary>
            Helper function to add given key/value pair as a new option.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.LoadSource(System.String,System.String[])">
            <summary>
            Helper function to create a DataFrame with the given source and paths.
            </summary>
            <param name="source">Name of the data source</param>
            <param name="paths">Input paths</param>
            <returns>A DataFrame object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameStatFunctions">
            <summary>
            Provides statistic functions for <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.ApproxQuantile(System.String,System.Collections.Generic.IEnumerable{System.Double},System.Double)">
            <summary>
            Calculates the approximate quantiles of a numerical column of a DataFrame.
            </summary>
            <remarks>
            This method implements a variation of the Greenwald-Khanna algorithm
            (with some speed optimizations).
            </remarks>
            <param name="columnName">Column name</param>
            <param name="probabilities">A list of quantile probabilities</param>
            <param name="relativeError">
            The relative target precision to achieve (greater than or equal to 0)
            </param>
            <returns>The approximate quantiles at the given probabilities</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.Cov(System.String,System.String)">
            <summary>
            Calculate the sample covariance of two numerical columns of a DataFrame.
            </summary>
            <param name="colName1">First column name</param>
            <param name="colName2">Second column name</param>
            <returns>The covariance of the two columns</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.Corr(System.String,System.String,System.String)">
            <summary>
            Calculates the correlation of two columns of a DataFrame.
            </summary>
            <remarks>
            Currently only the Pearson Correlation Coefficient is supported.
            </remarks>
            <param name="colName1">First column name</param>
            <param name="colName2">Second column name</param>
            <param name="method">Method name for calculating correlation</param>
            <returns>The Pearson Correlation Coefficient</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.Corr(System.String,System.String)">
            <summary>
            Calculates the Pearson Correlation Coefficient of two columns of a DataFrame.
            </summary>
            <param name="colName1">First column name</param>
            <param name="colName2">Second column name</param>
            <returns>The Pearson Correlation Coefficient</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.Crosstab(System.String,System.String)">
            <summary>
            Computes a pair-wise frequency table of the given columns, also known as 
            a contingency table.
            </summary>
            <param name="colName1">First column name</param>
            <param name="colName2">Second column name</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.FreqItems(System.Collections.Generic.IEnumerable{System.String},System.Double)">
            <summary>
            Finding frequent items for columns, possibly with false positives.
            </summary>
            <param name="columnNames">Column names</param>
            <param name="support">
            The minimum frequency for an item to be considered frequent.
            Should be greater than 1e-4.
            </param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.FreqItems(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Finding frequent items for columns, possibly with false positives with
            a default support of 1%.
            </summary>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.SampleBy``1(System.String,System.Collections.Generic.IDictionary{``0,System.Double},System.Int64)">
            <summary>
            Returns a stratified sample without replacement based on the fraction given
            on each stratum.
            </summary>
            <typeparam name="T">Stratum type</typeparam>
            <param name="columnName">Column name that defines strata</param>
            <param name="fractions">
            Sampling fraction for each stratum. If a stratum is not specified, we treat
            its fraction as zero.
            </param>
            <param name="seed">Random seed</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameWriter">
            <summary>
            Interface used to write a DataFrame to external storage systems (e.g. file systems,
            key-value stores, etc).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Mode(Microsoft.Spark.Sql.SaveMode)">
            <summary>
            Specifies the behavior when data or table already exists.
            </summary>
            <remarks>
            Options include:
              - SaveMode.Overwrite: overwrite the existing data.
              - SaveMode.Append: append the data.
              - SaveMode.Ignore: ignore the operation (i.e. no-op).
              - SaveMode.ErrorIfExists: default option, throw an exception at runtime.
            </remarks>
            <param name="saveMode">Save mode enum</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Mode(System.String)">
            <summary>
            Specifies the behavior when data or table already exists.
            </summary>
            <remarks>
            Options include:
              - "overwrite": overwrite the existing data.
              - "append": append the data.
              - "ignore": ignore the operation (i.e.no-op).
              - "error" or "errorifexists": default option, throw an exception at runtime.
            </remarks>
            <param name="saveMode">Save mode string</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Format(System.String)">
            <summary>
            Specifies the underlying output data source. Built-in options include
            "parquet", "json", etc.
            </summary>
            <param name="source">Data source name</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Option(System.String,System.String)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Option(System.String,System.Boolean)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Option(System.String,System.Int64)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Option(System.String,System.Double)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Options(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Adds output options for the underlying data source.
            </summary>
            <param name="options">Key/value options</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.PartitionBy(System.String[])">
            <summary>
            Partitions the output by the given columns on the file system. If specified,
            the output is laid out on the file system similar to Hive's partitioning scheme.
            </summary>
            <param name="colNames">Column names to partition by</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.BucketBy(System.Int32,System.String,System.String[])">
            <summary>
            Buckets the output by the given columns. If specified, the output is laid out
            on the file system similar to Hive's bucketing scheme.
            </summary>
            <param name="numBuckets">Number of buckets to save</param>
            <param name="colName">A column name</param>
            <param name="colNames">Additional column names</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.SortBy(System.String,System.String[])">
            <summary>
            Sorts the output in each bucket by the given columns.
            </summary>
            <param name="colName">A name of a column</param>
            <param name="colNames">Additional column names</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Save(System.String)">
            <summary>
            Saves the content of the DataFrame at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Save">
            <summary>
            Saves the content of the DataFrame as the specified table.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.InsertInto(System.String)">
            <summary>
            Inserts the content of the DataFrame to the specified table. It requires that
            the schema of the DataFrame is the same as the schema of the table.
            </summary>
            <param name="tableName">Name of the table</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.SaveAsTable(System.String)">
            <summary>
            Saves the content of the DataFrame as the specified table.
            </summary>
            <param name="tableName">Name of the table</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Jdbc(System.String,System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Saves the content of the DataFrame to a external database table via JDBC
            </summary>
            <param name="url">JDBC database URL of the form "jdbc:subprotocol:subname"</param>
            <param name="table">Name of the table in the external database</param>
            <param name="properties">JDBC database connection arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Json(System.String)">
            <summary>
            Saves the content of the DataFrame in JSON format at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Parquet(System.String)">
            <summary>
            Saves the content of the DataFrame in Parquet format at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Orc(System.String)">
            <summary>
            Saves the content of the DataFrame in ORC format at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Text(System.String)">
            <summary>
            Saves the content of the DataFrame in a text file at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Csv(System.String)">
            <summary>
            Saves the content of the DataFrame in CSV format at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.OptionInternal(System.String,System.Object)">
            <summary>
            Helper function to add given key/value pair as a new option.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Expressions.UserDefinedFunction">
            <summary>
            UserDefinedFunction is not exposed to the user directly.
            Use Functions.Udf to create this object indirectly.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Expressions.Window">
            <summary>
            Utility functions for defining window in DataFrames.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Expressions.Window.UnboundedPreceding">
            <summary>
            Value representing the first row in the partition, equivalent to
            "UNBOUNDED PRECEDING" in SQL.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Expressions.Window.UnboundedFollowing">
            <summary>
            Value representing the last row in the partition, equivalent to
            "UNBOUNDED FOLLOWING" in SQL.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Expressions.Window.CurrentRow">
            <summary>
            Value representing the current row.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.PartitionBy(System.String,System.String[])">
            <summary>
            Creates a `WindowSpec` with the partitioning defined.
            </summary>
            <param name="colName">Name of column</param>
            <param name="colNames">Additional column names</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.PartitionBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Creates a `WindowSpec` with the partitioning defined.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.OrderBy(System.String,System.String[])">
            <summary>
            Creates a `WindowSpec` with the ordering defined.
            </summary>
            <param name="colName">Name of column</param>
            <param name="colNames">Additional column names</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.OrderBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Creates a `WindowSpec` with the ordering defined.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.RowsBetween(System.Int64,System.Int64)">
            <summary>
            Creates a `WindowSpec` with the frame boundaries defined,
            from `start` (inclusive) to `end` (inclusive).
            </summary>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if this is
            the minimum long value `Window.s_unboundedPreceding`.
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if this is the
            maximum long value `Window.s_unboundedFollowing`.
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.RangeBetween(System.Int64,System.Int64)">
            <summary>
            Creates a `WindowSpec` with the frame boundaries defined,
            from `start` (inclusive) to `end` (inclusive).
            </summary>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if this is
            the minimum long value `Window.s_unboundedPreceding`.
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if this is
            maximum long value `Window.s_unboundedFollowing`.
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.RangeBetween(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a `WindowSpec` with the frame boundaries defined,
            from `start` (inclusive) to `end` (inclusive).
            </summary>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if the expression is
            `Microsoft.Spark.Sql.Functions.UnboundedPreceding()`
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if the expression is
            `Microsoft.Spark.Sql.Functions.UnboundedFollowing()`
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Expressions.WindowSpec">
            <summary>
            A window specification that defines the partitioning, ordering, and frame boundaries.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.PartitionBy(System.String,System.String[])">
            <summary>
            Defines the partitioning columns in a `WindowSpec`.
            </summary>
            <param name="colName">Name of column</param>
            <param name="colNames">Additional column names</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.PartitionBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Defines the partitioning columns in a `WindowSpec`.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.OrderBy(System.String,System.String[])">
            <summary>
            Defines the ordering columns in a `WindowSpec`.
            </summary>
            <param name="colName">Name of column</param>
            <param name="colNames">Additional column names</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.OrderBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Defines the ordering columns in a `WindowSpec`.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.RowsBetween(System.Int64,System.Int64)">
            <summary>
            Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).
            </summary>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if this is
            the minimum long value `Window.s_unboundedPreceding`.
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if this is the
            maximum long value `Window.s_unboundedFollowing`.
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.RangeBetween(System.Int64,System.Int64)">
            <summary>
            Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).
            </summary>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if this is
            the minimum long value `Window.s_unboundedPreceding`.
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if this is
            maximum long value `Window.s_unboundedFollowing`.
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.RangeBetween(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).
            </summary>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if the expression is
            `Microsoft.Spark.Sql.Functions.UnboundedPreceding()`
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if the expression is
            `Microsoft.Spark.Sql.Functions.UnboundedFollowing()`
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Functions">
            <summary>
            Functions available for DataFrame operations.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Column(System.String)">
            <summary>
            Returns a Column based on the given column name.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Col(System.String)">
            <summary>
            Returns a Column based on the given column name. Alias for Column().
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lit(System.Object)">
            <summary>
            Creates a Column of literal value.
            </summary>
            <param name="literal">Literal value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Asc(System.String)">
            <summary>
            Returns a sort expression based on the ascending order of the column.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.AscNullsFirst(System.String)">
            <summary>
            Returns a sort expression based on the ascending order of the column,
            and null values return before non-null values.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.AscNullsLast(System.String)">
            <summary>
            Returns a sort expression based on the ascending order of the column,
            and null values appear after non-null values.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Desc(System.String)">
            <summary>
            Returns a sort expression based on the descending order of the column.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DescNullsFirst(System.String)">
            <summary>
            Returns a sort expression based on the descending order of the column,
            and null values return before non-null values.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DescNullsLast(System.String)">
            <summary>
            Returns a sort expression based on the descending order of the column,
            and null values appear after non-null values.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ApproxCountDistinct(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the approximate number of distinct items in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ApproxCountDistinct(System.String)">
            <summary>
            Returns the approximate number of distinct items in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ApproxCountDistinct(Microsoft.Spark.Sql.Column,System.Double)">
            <summary>
            Returns the approximate number of distinct items in a group.
            </summary>
            <param name="column">Column to apply</param>
            <param name="rsd">Maximum estimation error allowed</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ApproxCountDistinct(System.String,System.Double)">
            <summary>
            Returns the approximate number of distinct items in a group.
            </summary>
            <param name="columnName">Column name</param>
            <param name="rsd">Maximum estimation error allowed</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Avg(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the average of the values in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Avg(System.String)">
            <summary>
            Returns the average of the values in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CollectList(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a list of objects with duplicates.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CollectList(System.String)">
            <summary>
            Returns a list of objects with duplicates.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CollectSet(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a set of objects with duplicate elements eliminated.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CollectSet(System.String)">
            <summary>
            Returns a set of objects with duplicate elements eliminated.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Corr(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the Pearson Correlation Coefficient for two columns.
            </summary>
            <param name="column1">Column one to apply</param>
            <param name="column2">Column two to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Corr(System.String,System.String)">
            <summary>
            Returns the Pearson Correlation Coefficient for two columns.
            </summary>
            <param name="columnName1">Column one name</param>
            <param name="columnName2">Column two name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Count(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the number of items in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Count(System.String)">
            <summary>
            Returns the number of items in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CountDistinct(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the number of distinct items in a group.
            </summary>
            <param name="column">Column to apply</param>
            <param name="columns">Additional columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CountDistinct(System.String,System.String[])">
            <summary>
            Returns the number of distinct items in a group.
            </summary>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CovarPop(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the population covariance for two columns.
            </summary>
            <param name="column1">Column one to apply</param>
            <param name="column2">Column two to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CovarPop(System.String,System.String)">
            <summary>
            Returns the population covariance for two columns.
            </summary>
            <param name="columnName1">Column one name</param>
            <param name="columnName2">Column two name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CovarSamp(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the sample covariance for two columns.
            </summary>
            <param name="column1">Column one to apply</param>
            <param name="column2">Column two to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CovarSamp(System.String,System.String)">
            <summary>
            Returns the sample covariance for two columns.
            </summary>
            <param name="columnName1">Column one name</param>
            <param name="columnName2">Column two name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.First(Microsoft.Spark.Sql.Column,System.Boolean)">
            <summary>
            Returns the first value of a column in a group.
            </summary>
            <remarks>
            The function by default returns the first values it sees. It will return
            the first non-null value it sees when ignoreNulls is set to true.
            If all values are null, then null is returned.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="ignoreNulls">To ignore null or not</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.First(System.String,System.Boolean)">
            <summary>
            Returns the first value of a column in a group.
            </summary>
            <remarks>
            The function by default returns the first values it sees. It will return
            the first non-null value it sees when ignoreNulls is set to true.
            If all values are null, then null is returned.
            </remarks>
            <param name="columnName">Column name</param>
            <param name="ignoreNulls">To ignore null or not</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Grouping(Microsoft.Spark.Sql.Column)">
            <summary>
            Indicates whether a specified column in a GROUP BY list is aggregated
            or not, returning 1 for aggregated or 0 for not aggregated in the result set.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Grouping(System.String)">
            <summary>
            Indicates whether a specified column in a GROUP BY list is aggregated
            or not, returning 1 for aggregated or 0 for not aggregated in the result set.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.GroupingId(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the number of distinct items in a group.
            </summary>
            <remarks>
            The list of columns should match with grouping columns exactly, or empty
            (meaning all the grouping columns).
            </remarks>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.GroupingId(System.String,System.String[])">
            <summary>
            Returns the number of distinct items in a group.
            </summary>
            <remarks>
            The list of columns should match with grouping columns exactly.
            </remarks>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Kurtosis(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the kurtosis of the values in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Kurtosis(System.String)">
            <summary>
            Returns the kurtosis of the values in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Last(Microsoft.Spark.Sql.Column,System.Boolean)">
            <summary>
            Returns the last value of a column in a group.
            </summary>
            <remarks>
            The function by default returns the last values it sees. It will return
            the last non-null value it sees when ignoreNulls is set to true.
            If all values are null, then null is returned.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="ignoreNulls">To ignore null or not</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Last(System.String,System.Boolean)">
            <summary>
            Returns the last value of a column in a group.
            </summary>
            <remarks>
            The function by default returns the last values it sees. It will return
            the last non-null value it sees when ignoreNulls is set to true.
            If all values are null, then null is returned.
            </remarks>
            <param name="columnName">Column name</param>
            <param name="ignoreNulls">To ignore null or not</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Max(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the maximum value of the column in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Max(System.String)">
            <summary>
            Returns the maximum value of the column in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Mean(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the average value of the column in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Mean(System.String)">
            <summary>
            Returns the average value of the column in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Min(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the minimum value of the column in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Min(System.String)">
            <summary>
            Returns the minimum value of the column in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Skewness(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the skewness of the values in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Skewness(System.String)">
            <summary>
            Returns the skewness of the values in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Stddev(Microsoft.Spark.Sql.Column)">
            <summary>
            Alias for StddevSamp().
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Stddev(System.String)">
            <summary>
            Alias for StddevSamp().
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.StddevSamp(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the sample standard deviation of the expression in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.StddevSamp(System.String)">
            <summary>
            Returns the sample standard deviation of the expression in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.StddevPop(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the population standard deviation of the expression in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.StddevPop(System.String)">
            <summary>
            Returns the population standard deviation of the expression in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sum(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the sum of all values in the expression.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sum(System.String)">
            <summary>
            Returns the sum of all values in the expression.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SumDistinct(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the sum of distinct values in the expression.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SumDistinct(System.String)">
            <summary>
            Returns the sum of distinct values in the expression.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Variance(Microsoft.Spark.Sql.Column)">
            <summary>
            Alias for VarSamp().
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Variance(System.String)">
            <summary>
            Alias for VarSamp().
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VarSamp(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the unbiased variance of the values in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VarSamp(System.String)">
            <summary>
            Returns the unbiased variance of the values in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VarPop(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the population variance of the values in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VarPop(System.String)">
            <summary>
            Returns the population variance of the values in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.UnboundedPreceding">
            <summary>
            Window function: returns the special frame boundary that represents the first
            row in the window partition.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.UnboundedFollowing">
            <summary>
            Window function: returns the special frame boundary that represents the last
            row in the window partition.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CurrentRow">
            <summary>
            Window function: returns the special frame boundary that represents the current
            row in the window partition.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CumeDist">
            <summary>
            Window function: returns the cumulative distribution of values within a window
            partition, i.e. the fraction of rows that are below the current row.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DenseRank">
            <summary>
            Window function: returns the rank of rows within a window partition, without any gaps.
            </summary>
            <remarks>This is equivalent to the DENSE_RANK function in SQL.</remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lag(Microsoft.Spark.Sql.Column,System.Int32,System.Object)">
            <summary>
            Window function: returns the value that is 'offset' rows before the current row,
            and null if there is less than 'offset' rows before the current row.
            For example, an 'offset' of one will return the previous row at any given point
            in the window partition.
            </summary>
            <remarks>This is equivalent to the LAG function in SQL.</remarks>
            <param name="column">Column to apply</param>
            <param name="offset">Offset from the current row</param>
            <param name="defaultValue">Default value when the offset row doesn't exist</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lag(System.String,System.Int32,System.Object)">
            <summary>
            Window function: returns the value that is 'offset' rows before the current row,
            and null if there is less than 'offset' rows before the current row.
            For example, an 'offset' of one will return the previous row at any given point
            in the window partition.
            </summary>
            <remarks>This is equivalent to the LAG function in SQL.</remarks>
            <param name="columnName">Column name</param>
            <param name="offset">Offset from the current row</param>
            <param name="defaultValue">Default value when the offset row doesn't exist</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lead(Microsoft.Spark.Sql.Column,System.Int32,System.Object)">
            <summary>
            Window function: returns the value that is 'offset' rows after the current row,
            and null if there is less than 'offset' rows after the current row.
            For example, an 'offset' of one will return the next row at any given point
            in the window partition.
            </summary>
            <remarks>This is equivalent to the LEAD function in SQL.</remarks>
            <param name="column">Column to apply</param>
            <param name="offset">Offset from the current row</param>
            <param name="defaultValue">Default value when the offset row doesn't exist</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lead(System.String,System.Int32,System.Object)">
            <summary>
            Window function: returns the value that is 'offset' rows after the current row,
            and null if there is less than 'offset' rows after the current row.
            For example, an 'offset' of one will return the next row at any given point
            in the window partition.
            </summary>
            <remarks>This is equivalent to the LEAD function in SQL.</remarks>
            <param name="columnName">Column name</param>
            <param name="offset">Offset from the current row</param>
            <param name="defaultValue">Default value when the offset row doesn't exist</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ntile(System.Int32)">
            <summary>
            Window function: returns the ntile group id (from 1 to `n` inclusive) in an ordered
            window partition. For example, if `n` is 4, the first quarter of the rows will get
            value 1, the second quarter will get 2, the third quarter will get 3, and the last
            quarter will get 4.
            </summary>
            <remarks>This is equivalent to the NTILE function in SQL.</remarks>
            <param name="n">Number of buckets</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.PercentRank">
            <summary>
            Window function: returns the relative rank (i.e. percentile) of rows within
            a window partition.
            </summary>
            <remarks>This is equivalent to the PERCENT_RANK function in SQL.</remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rank">
            <summary>
            Window function: returns the rank of rows within a window partition.
            </summary>
            <remarks>This is equivalent to the RANK function in SQL.</remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.RowNumber">
            <summary>
            Window function: returns a sequential number starting at 1 within a window partition.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Abs(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the absolute value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Array(Microsoft.Spark.Sql.Column[])">
            <summary>
            Creates a new array column. The input columns must all have the same data type.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Array(System.String,System.String[])">
            <summary>
            Creates a new array column. The input columns must all have the same data type.
            </summary>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Map(Microsoft.Spark.Sql.Column[])">
            <summary>
            Creates a new map column.
            </summary>
            <remarks>
            The input columns must be grouped as key-value pairs, e.g.
            (key1, value1, key2, value2, ...). The key columns must all have the same data type,
            and can't be null. The value columns must all have the same data type.
            </remarks>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MapFromArrays(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a new map column. The array in the first column is used for keys. The array
            in the second column is used for values. All elements in the array for key should
            not be null.
            </summary>
            <param name="key">Column expression for key</param>
            <param name="values">Column expression for values</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Broadcast(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Marks a DataFrame as small enough for use in broadcast joins.
            </summary>
            <param name="df">DataFrame to apply</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Coalesce(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the first column that is not null, or null if all inputs are null.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.InputFileName">
            <summary>
            Creates a string column for the file name of the current Spark task.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.IsNaN(Microsoft.Spark.Sql.Column)">
            <summary>
            Return true iff the column is NaN.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.IsNull(Microsoft.Spark.Sql.Column)">
            <summary>
            Return true iff the column is null.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MonotonicallyIncreasingId">
            <summary>
            A column expression that generates monotonically increasing 64-bit integers.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.NaNvl(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns col1 if it is not NaN, or col2 if col1 is NaN.
            </summary>
            <remarks>
            Both inputs should be floating point columns (DoubleType or FloatType).
            </remarks>
            <param name="column1">Column one to apply</param>
            <param name="column2">Column two to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Negate(Microsoft.Spark.Sql.Column)">
            <summary>
            Unary minus, i.e. negate the expression.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Not(Microsoft.Spark.Sql.Column)">
            <summary>
            Inversion of boolean expression, i.e. NOT.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rand(System.Int64)">
            <summary>
            Generate a random column with independent and identically distributed (i.i.d.)
            samples from U[0.0, 1.0].
            </summary>
            <remarks>
            This is non-deterministic when data partitions are not fixed.
            </remarks>
            <param name="seed">Random seed</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rand">
            <summary>
            Generate a random column with independent and identically distributed (i.i.d.)
            samples from U[0.0, 1.0].
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Randn(System.Int64)">
            <summary>
            Generate a random column with independent and identically distributed (i.i.d.)
            samples from the standard normal distribution.
            </summary>
            <remarks>
            This is non-deterministic when data partitions are not fixed.
            </remarks>
            <param name="seed">Random seed</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Randn">
            <summary>
            Generate a random column with independent and identically distributed (i.i.d.)
            samples from the standard normal distribution.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SparkPartitionId">
            <summary>
            Partition ID.
            </summary>
            <remarks>
            This is non-deterministic because it depends on data partitioning and task scheduling.
            </remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sqrt(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the square root of the specified float value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sqrt(System.String)">
            <summary>
            Computes the square root of the specified float value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Struct(Microsoft.Spark.Sql.Column[])">
            <summary>
            Creates a new struct column that composes multiple input columns.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Struct(System.String,System.String[])">
            <summary>
            Creates a new struct column that composes multiple input columns.
            </summary>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.When(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Evaluates a condition and returns one of multiple possible result expressions.
            If otherwise is not defined at the end, null is returned for
            unmatched conditions.
            </summary>
            <param name="condition">The condition to check.</param>
            <param name="value">The value to set if the condition is true.</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.BitwiseNOT(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes bitwise NOT.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Expr(System.String)">
            <summary>
            Parses the expression string into the column that it represents.
            </summary>
            <param name="expr">Expression string</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Acos(Microsoft.Spark.Sql.Column)">
            <summary>
            Inverse cosine of `column` in radians, as if computed by `java.lang.Math.acos`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Acos(System.String)">
            <summary>
            Inverse cosine of `columnName` in radians, as if computed by `java.lang.Math.acos`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Asin(Microsoft.Spark.Sql.Column)">
            <summary>
            Inverse sine of `column` in radians, as if computed by `java.lang.Math.asin`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Asin(System.String)">
            <summary>
            Inverse sine of `columnName` in radians, as if computed by `java.lang.Math.asin`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan(Microsoft.Spark.Sql.Column)">
            <summary>
            Inverse tangent of `column` in radians, as if computed by `java.lang.Math.atan`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan(System.String)">
            <summary>
            Inverse tangent of `columnName` in radians, as if computed by `java.lang.Math.atan`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="y">Coordinate on y-axis</param>
            <param name="x">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="y">Coordinate on y-axis</param>
            <param name="xName">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="yName">Coordinate on y-axis</param>
            <param name="x">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(System.String,System.String)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="yName">Coordinate on y-axis</param>
            <param name="xName">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(Microsoft.Spark.Sql.Column,System.Double)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="y">Coordinate on y-axis</param>
            <param name="xValue">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(System.String,System.Double)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="yName">Coordinate on y-axis</param>
            <param name="xValue">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(System.Double,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="yValue">Coordinate on y-axis</param>
            <param name="x">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(System.Double,System.String)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="yValue">Coordinate on y-axis</param>
            <param name="xName">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Bin(Microsoft.Spark.Sql.Column)">
            <summary>
            An expression that returns the string representation of the binary value
            of the given long column. For example, bin("12") returns "1100".
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Bin(System.String)">
            <summary>
            An expression that returns the string representation of the binary value
            of the given long column. For example, bin("12") returns "1100".
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cbrt(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the cube-root of the given column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cbrt(System.String)">
            <summary>
            Computes the cube-root of the given column.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ceil(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the ceiling of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ceil(System.String)">
            <summary>
            Computes the ceiling of the given value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Conv(Microsoft.Spark.Sql.Column,System.Int32,System.Int32)">
            <summary>
            Convert a number in a string column from one base to another.
            </summary>
            <param name="column">Column to apply</param>
            <param name="fromBase">Source base number</param>
            <param name="toBase">Target base number</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cos(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes cosine of the angle, as if computed by `java.lang.Math.cos`
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cos(System.String)">
            <summary>
            Computes cosine of the angle, as if computed by `java.lang.Math.cos`
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cosh(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh`
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cosh(System.String)">
            <summary>
            Computes hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh`
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Exp(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the exponential of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Exp(System.String)">
            <summary>
            Computes the exponential of the given value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Expm1(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the exponential of the given value minus one.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Expm1(System.String)">
            <summary>
            Computes the exponential of the given value minus one.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Factorial(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the factorial of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Floor(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the floor of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Floor(System.String)">
            <summary>
            Computes the floor of the given value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Greatest(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the greatest value of the list of values, skipping null values.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Greatest(System.String,System.String[])">
            <summary>
            Returns the greatest value of the list of column names, skipping null values.
            </summary>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hex(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes hex value of the given column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Unhex(Microsoft.Spark.Sql.Column)">
            <summary>
            Inverse of hex. Interprets each pair of characters as a hexadecimal number
            and converts to the byte representation of number.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(System.String,System.String)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(Microsoft.Spark.Sql.Column,System.Double)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(System.String,System.Double)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="right">Right side value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(System.Double,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="left">Left side value</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(System.Double,System.String)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="left">Left side value</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Least(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the least value of the list of values, skipping null values.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Least(System.String,System.String[])">
            <summary>
            Returns the least value of the list of values, skipping null values.
            </summary>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the natural logarithm of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log(System.String)">
            <summary>
            Computes the natural logarithm of the given value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log(System.Double,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the first argument-base logarithm of the second argument.
            </summary>
            <param name="logBase">Base for logarithm</param>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log(System.Double,System.String)">
            <summary>
            Computes the first argument-base logarithm of the second argument.
            </summary>
            <param name="logBase">Base for logarithm</param>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log10(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the logarithm of the given value in base 10.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log10(System.String)">
            <summary>
            Computes the logarithm of the given value in base 10.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log1p(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the natural logarithm of the given value plus one.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log1p(System.String)">
            <summary>
            Computes the natural logarithm of the given value plus one.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log2(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the logarithm of the given column in base 2.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log2(System.String)">
            <summary>
            Computes the logarithm of the given column in base 2.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(System.String,System.String)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(Microsoft.Spark.Sql.Column,System.Double)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(System.String,System.Double)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="right">Right side value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(System.Double,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="left">Left side value</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(System.Double,System.String)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="left">Left side value</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pmod(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the positive value of dividend mod divisor.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rint(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the double value that is closest in value to the argument and
            is equal to a mathematical integer.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rint(System.String)">
            <summary>
            Returns the double value that is closest in value to the argument and
            is equal to a mathematical integer.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Round(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the value of the `column` rounded to 0 decimal places with
            HALF_UP round mode.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Round(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Returns the value of the `column` rounded to `scale` decimal places with
            HALF_UP round mode.
            </summary>
            <param name="column">Column to apply</param>
            <param name="scale">Scale factor</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Bround(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the value of the `column` rounded to 0 decimal places with
            HALF_EVEN round mode.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Bround(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Returns the value of the `column` rounded to `scale` decimal places with
            HALF_EVEN round mode.
            </summary>
            <param name="column">Column to apply</param>
            <param name="scale">Scale factor</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ShiftLeft(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Shift the given value `numBits` left.
            </summary>
            <param name="column">Column to apply</param>
            <param name="numBits">Number of bits to shift</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ShiftRight(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            (Signed) shift the given value `numBits` right.
            </summary>
            <param name="column">Column to apply</param>
            <param name="numBits">Number of bits to shift</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ShiftRightUnsigned(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Unsigned shift the given value `numBits` right.
            </summary>
            <param name="column">Column to apply</param>
            <param name="numBits">Number of bits to shift</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Signum(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the signum of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Signum(System.String)">
            <summary>
            Computes the signum of the given value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sin(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes sine of the angle, as if computed by `java.lang.Math.sin`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sin(System.String)">
            <summary>
            Computes sine of the angle, as if computed by `java.lang.Math.sin`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sinh(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes hyperbolic sine of the angle, as if computed by `java.lang.Math.sin`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sinh(System.String)">
            <summary>
            Computes hyperbolic sine of the angle, as if computed by `java.lang.Math.sin`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Tan(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes tangent of the given value, as if computed by `java.lang.Math.tan`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Tan(System.String)">
            <summary>
            Computes tangent of the given value, as if computed by `java.lang.Math.tan`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Tanh(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes hyperbolic tangent of the given value, as if computed by
            `java.lang.Math.tanh`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Tanh(System.String)">
            <summary>
            Computes hyperbolic tangent of the given value, as if computed by
            `java.lang.Math.tanh`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Degrees(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts an angle measured in radians to an approximately equivalent angle
            measured in degrees.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Degrees(System.String)">
            <summary>
            Converts an angle measured in radians to an approximately equivalent angle
            measured in degrees.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Radians(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts an angle measured in degrees to an approximately equivalent angle
            measured in radians.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Radians(System.String)">
            <summary>
            Converts an angle measured in degrees to an approximately equivalent angle
            measured in radians.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Md5(Microsoft.Spark.Sql.Column)">
            <summary>
            Calculates the MD5 digest of a binary column and returns the value
            as a 32 character hex string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sha1(Microsoft.Spark.Sql.Column)">
            <summary>
            Calculates the SHA-1 digest of a binary column and returns the value
            as a 40 character hex string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sha2(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Calculates the SHA-2 family of hash functions of a binary column and
            returns the value as a hex string.
            </summary>
            <param name="column">Column to apply</param>
            <param name="numBits">One of 224, 256, 384 or 512</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Crc32(Microsoft.Spark.Sql.Column)">
            <summary>
            Calculates the cyclic redundancy check value  (CRC32) of a binary column and
            returns the value as a bigint.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hash(Microsoft.Spark.Sql.Column[])">
            <summary>
            Calculates the hash code of given columns, and returns the result as an int column.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ascii(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the numeric value of the first character of the string column, and returns
            the result as an int column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Base64(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the BASE64 encoding of a binary column and returns it as a string column.
            </summary>
            <remarks>
            This is the reverse of unbase64.
            </remarks>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ConcatWs(System.String,Microsoft.Spark.Sql.Column[])">
            <summary>
            Concatenates multiple input string columns together into a single string column,
            using the given separator.
            </summary>
            <param name="sep">Separator used for string concatenation</param>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Decode(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Computes the first argument into a string from a binary using the provided
            character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE',
            'UTF-16')
            </summary>
            <param name="column">Column to apply</param>
            <param name="charset">Character set</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Encode(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Computes the first argument into a binary from a string using the provided
            character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE',
            'UTF-16')
            </summary>
            <param name="column">Column to apply</param>
            <param name="charset">Character set</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FormatNumber(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Formats the given numeric `column` to a format like '#,###,###.##',
            rounded to the given `d` decimal places with HALF_EVEN round mode,
            and returns the result as a string column.
            </summary>
            <param name="column">Column to apply</param>
            <param name="d">Decimal places for rounding</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FormatString(System.String,Microsoft.Spark.Sql.Column[])">
            <summary>
            Formats the arguments in printf-style and returns the result as a string column.
            </summary>
            <param name="format">Printf-style format</param>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.InitCap(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a new string column by converting the first letter of each word to uppercase.
            Words are delimited by whitespace.
            </summary>
            <remarks>
            </remarks>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Instr(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Locate the position of the first occurrence of the given substring.
            </summary>
            <remarks>
            The position is not zero based, but 1 based index. Returns 0 if the given substring
            could not be found.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="substring">Substring to find</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Length(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the character length of a given string or number of bytes of a binary string.
            </summary>
            <remarks>
            The length of character strings includes the trailing spaces. The length of binary
            strings includes binary zeros.
            </remarks>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lower(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts a string column to lower case.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Levenshtein(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the Levenshtein distance of the two given string columns.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Locate(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Locate the position of the first occurrence of the given substring.
            </summary>
            <remarks>
            The position is not zero based, but 1 based index. Returns 0 if the given substring
            could not be found.
            </remarks>
            <param name="substring">Substring to find</param>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Locate(System.String,Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Locate the position of the first occurrence of the given substring
            starting from the given position offset.
            </summary>
            <remarks>
            The position is not zero based, but 1 based index. Returns 0 if the given substring
            could not be found.
            </remarks>
            <param name="substring">Substring to find</param>
            <param name="column">Column to apply</param>
            <param name="pos">Offset to start the search</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lpad(Microsoft.Spark.Sql.Column,System.Int32,System.String)">
            <summary>
            Left-pad the string column with pad to the given length `len`. If the string column is
            longer than `len`, the return value is shortened to `len` characters.
            </summary>
            <param name="column">Column to apply</param>
            <param name="len">Length of padded string</param>
            <param name="pad">String used for padding</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ltrim(Microsoft.Spark.Sql.Column)">
            <summary>
            Trim the spaces from left end for the given string column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ltrim(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Trim the specified character string from left end for the given string column.
            </summary>
            <param name="column">Column to apply</param>
            <param name="trimString">String to trim</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.RegexpExtract(Microsoft.Spark.Sql.Column,System.String,System.Int32)">
            <summary>
            Extract a specific group matched by a Java regex, from the specified string column.
            </summary>
            <remarkes>
            If the regex did not match, or the specified group did not match,
            an empty string is returned.
            </remarkes>
            <param name="column">Column to apply</param>
            <param name="exp">Regular expression to match</param>
            <param name="groupIdx">Group index to extract</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.RegexpReplace(Microsoft.Spark.Sql.Column,System.String,System.String)">
            <summary>
            Replace all substrings of the specified string value that match the pattern with
            the given replacement string.
            </summary>
            <param name="column">Column to apply</param>
            <param name="pattern">Regular expression to match</param>
            <param name="replacement">String to replace with</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.RegexpReplace(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Replace all substrings of the specified string value that match the pattern with
            the given replacement string.
            </summary>
            <param name="column">Column to apply</param>
            <param name="pattern">Regular expression to match</param>
            <param name="replacement">String to replace with</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Unbase64(Microsoft.Spark.Sql.Column)">
            <summary>
            Decodes a BASE64 encoded string column and returns it as a binary column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rpad(Microsoft.Spark.Sql.Column,System.Int32,System.String)">
            <summary>
            Right-pad the string column with pad to the given length `len`. If the string column is
            longer than `len`, the return value is shortened to `len` characters.
            </summary>
            <param name="column">Column to apply</param>
            <param name="len">Length of padded string</param>
            <param name="pad">String used for padding</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Repeat(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Repeats a string column `n` times, and returns it as a new string column.
            </summary>
            <param name="column">Column to apply</param>
            <param name="n">Repeatation number</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rtrim(Microsoft.Spark.Sql.Column)">
            <summary>
            Trim the spaces from right end for the specified string value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rtrim(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Trim the specified character string from right end for the given string column.
            </summary>
            <param name="column">Column to apply</param>
            <param name="trimString">String to trim</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Soundex(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the soundex code for the specified expression.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Split(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Splits string with a regular expression pattern.
            </summary>
            <param name="column">Column to apply</param>
            <param name="pattern">Regular expression pattern</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Substring(Microsoft.Spark.Sql.Column,System.Int32,System.Int32)">
            <summary>
            Returns the substring (or slice of byte array) starting from the given
            position for the given length.
            </summary>
            <remarks>
            The position is not zero based, but 1 based index.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="pos">Starting position</param>
            <param name="len">Length of the substring</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SubstringIndex(Microsoft.Spark.Sql.Column,System.String,System.Int32)">
            <summary>
            Returns the substring from the given string before `count` occurrences of
            the given delimiter.
            </summary>
            <param name="column">Column to apply</param>
            <param name="delimiter">Delimiter to find</param>
            <param name="count">Number of occurrences of delimiter</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Translate(Microsoft.Spark.Sql.Column,System.String,System.String)">
            <summary>
            Translate any characters that match with the given `matchingString` in the column
            by the given `replaceString`.
            </summary>
            <param name="column">Column to apply</param>
            <param name="matchingString">String to match</param>
            <param name="replaceString">String to replace with</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Trim(Microsoft.Spark.Sql.Column)">
            <summary>
            Trim the spaces from both ends for the specified string column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Trim(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Trim the specified character from both ends for the specified string column.
            </summary>
            <param name="column">Column to apply</param>
            <param name="trimString">String to trim</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Upper(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts a string column to upper case.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.AddMonths(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Returns the date that is `numMonths` after `startDate`.
            </summary>
            <param name="startDate">Start date</param>
            <param name="numMonths">Number of months to add to start date</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CurrentDate">
            <summary>
            Returns the current date as a date column.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CurrentTimestamp">
            <summary>
            Returns the current timestamp as a timestamp column.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DateFormat(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Converts a date/timestamp/string to a value of string in the format specified
            by the date format given by the second argument.
            </summary>
            <param name="dateExpr">Date expression</param>
            <param name="format">Format string to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DateAdd(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Returns the date that is `days` days after `start`.
            </summary>
            <param name="start">Start date</param>
            <param name="days">Number of days to add to start data</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DateSub(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Returns the date that is `days` days before `start`.
            </summary>
            <param name="start">Start date</param>
            <param name="days">Number of days to subtract from start data</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DateDiff(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the number of days from `start` to `end`.
            </summary>
            <param name="start">Start date</param>
            <param name="end">End date</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Year(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the year as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Quarter(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the quarter as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Month(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the month as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DayOfWeek(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the day of the week as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DayOfMonth(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the day of the month as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DayOfYear(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the day of the year as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hour(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the hours as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.LastDay(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the last day of the month which the given date belongs to.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Minute(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the minutes as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MonthsBetween(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns number of months between dates `end` and `stasrt`.
            </summary>
            <param name="end">Date column</param>
            <param name="start">Date column</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MonthsBetween(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column,System.Boolean)">
            <summary>
            Returns number of months between dates `end` and `start`. If `roundOff` is set to true,
            the result is rounded off to 8 digits; it is not rounded otherwise.
            </summary>
            <param name="end">Date column</param>
            <param name="start">Date column</param>
            <param name="roundOff">To round or not</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.NextDay(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Given a date column, returns the first date which is later than the value of
            the date column that is on the specified day of the week.
            </summary>
            <param name="date">Date column</param>
            <param name="dayOfWeek">
            One of the following (case-insensitive):
              "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun".
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Second(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the seconds as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.WeekOfYear(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the week number as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromUnixTime(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts the number of seconds from UNIX epoch (1970-01-01 00:00:00 UTC) to a string
            representing the timestamp of that moment in the current system time zone with
            a default format "yyyy-MM-dd HH:mm:ss".
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromUnixTime(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Converts the number of seconds from UNIX epoch (1970-01-01 00:00:00 UTC) to a string
            representing the timestamp of that moment in the current system time zone with
            the given format.
            </summary>
            <param name="column">Column to apply</param>
            <param name="format">Format of the timestamp</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.UnixTimestamp">
            <summary>
            Returns the current Unix timestamp (in seconds).
            </summary>
            <remarks>
            All calls of `UnixTimestamp` within the same query return the same value
            (i.e. the current timestamp is calculated at the start of query evaluation).
            </remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.UnixTimestamp(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds),
            using the default timezone and the default locale.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.UnixTimestamp(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Converts time string with given format to Unix timestamp (in seconds).
            </summary>
            <remarks>
            Supported date format can be found:
            http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html
            </remarks>
            <param name="column">Column to apply</param>
            <param name="format">Date format</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToTimestamp(Microsoft.Spark.Sql.Column)">
            <summary>
            Convert time string to a Unix timestamp (in seconds) by casting rules to
            `TimestampType`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToTimestamp(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Convert time string to a Unix timestamp (in seconds) with specified format.
            </summary>
            <remarks>
            Supported date format can be found:
            http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html
            </remarks>
            <param name="column">Column to apply</param>
            <param name="format">Date format</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToDate(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts the column into `DateType` by casting rules to `DateType`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToDate(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Converts the column into a `DateType` with a specified format.
            </summary>
            <remarks>
            Supported date format can be found:
            http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html
            </remarks>
            <param name="column">Column to apply</param>
            <param name="format">Date format</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Trunc(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Returns date truncated to the unit specified by the format.
            </summary>
            <param name="column">Column to apply</param>
            <param name="format">
            'year', 'yyyy', 'yy' for truncate by year, or
            'month', 'mon', 'mm' for truncate by month
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DateTrunc(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns timestamp truncated to the unit specified by the format.
            </summary>
            <param name="format">
            'year', 'yyyy', 'yy' for truncate by year, or
            'month', 'mon', 'mm' for truncate by month, or
            'day', 'dd' for truncate by day, or
            'second', 'minute', 'hour', 'week', 'month', 'quarter'
            </param>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromUtcTimestamp(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC,
            and renders that time as a timestamp in the given time zone. For example, 'GMT+1'
            would yield '2017-07-14 03:40:00.0'.
            </summary>
            <param name="column">Column to apply</param>
            <param name="tz">Timezone string</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromUtcTimestamp(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC,
            and renders that time as a timestamp in the given time zone. For example, 'GMT+1'
            would yield '2017-07-14 03:40:00.0'.
            </summary>
            <param name="column">Column to apply</param>
            <param name="tz">Timezone expression</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToUtcTimestamp(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the
            given time zone, and renders that time as a timestamp in UTC. For example, 'GMT+1'
            would yield '2017-07-14 01:40:00.0'.
            </summary>
            <param name="column">Column to apply</param>
            <param name="tz">Timezone string</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToUtcTimestamp(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the
            given time zone, and renders that time as a timestamp in UTC. For example, 'GMT+1'
            would yield '2017-07-14 01:40:00.0'.
            </summary>
            <param name="column">Column to apply</param>
            <param name="tz">Timezone expression</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Window(Microsoft.Spark.Sql.Column,System.String,System.String,System.String)">
            <summary>
            Bucketize rows into one or more time windows given a timestamp column.
            </summary>
            <remarks>
            Refer to org.apache.spark.unsafe.types.CalendarInterval for the duration strings.
            </remarks>
            <param name="column">The column to use as the timestamp for windowing by time</param>
            <param name="windowDuration">A string specifying the width of the window</param>
            <param name="slideDuration">
            A string specifying the sliding interval of the window
            </param>
            <param name="startTime">
            The offset with respect to 1970-01-01 00:00:00 UTC with which to start window intervals
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Window(Microsoft.Spark.Sql.Column,System.String,System.String)">
            <summary>
            Bucketize rows into one or more time windows given a timestamp column.
            </summary>
            <remarks>
            Refer to org.apache.spark.unsafe.types.CalendarInterval for the duration strings.
            </remarks>
            <param name="column">The column to use as the timestamp for windowing by time</param>
            <param name="windowDuration">A string specifying the width of the window</param>
            <param name="slideDuration">
            A string specifying the sliding interval of the window
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Window(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Generates tumbling time windows given a timestamp specifying column.
            </summary>
            <param name="column">The column to use as the timestamp for windowing by time</param>
            <param name="windowDuration">
            A string specifying the width of the window.
            Refer to org.apache.spark.unsafe.types.CalendarInterval for the duration strings.
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayContains(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Returns null if the array is null, true if the array contains `value`,
            and false otherwise.
            </summary>
            <param name="column">Column to apply</param>
            <param name="value">Value to check for existence</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArraysOverlap(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns true if `a1` and `a2` have at least one non-null element in common.
            If not and both arrays are non-empty and any of them contains a null,
            it returns null. It returns false otherwise.
            </summary>
            <param name="a1">Left side array</param>
            <param name="a2">Right side array</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Slice(Microsoft.Spark.Sql.Column,System.Int32,System.Int32)">
            <summary>
            Returns an array containing all the elements in `column` from index `start`
            (or starting from the end if `start` is negative) with the specified `length`.
            </summary>
            <param name="column">Column to apply</param>
            <param name="start">Start position in the array</param>
            <param name="length">Length for slicing</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayJoin(Microsoft.Spark.Sql.Column,System.String,System.String)">
            <summary>
            Concatenates the elements of `column` using the `delimiter`.
            Null values are replaced with `nullReplacement`.
            </summary>
            <param name="column">Column to apply</param>
            <param name="delimiter">Delimiter for join</param>
            <param name="nullReplacement">String to replace null value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayJoin(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Concatenates the elements of `column` using the `delimiter`.
            </summary>
            <param name="column">Column to apply</param>
            <param name="delimiter">Delimiter for join</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Concat(Microsoft.Spark.Sql.Column[])">
            <summary>
            Concatenates multiple input columns together into a single column.
            </summary>
            <remarks>
            If all inputs are binary, concat returns an output as binary.
            Otherwise, it returns as string.
            </remarks>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayPosition(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Locates the position of the first occurrence of the value in the given array as long.
            Returns null if either of the arguments are null.
            </summary>
            <remarks>
            The position is not zero based, but 1 based index.
            Returns 0 if value could not be found in array.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="value">Value to locate</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ElementAt(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Returns element of array at given index in `value` if column is array.
            Returns value for the given key in `value` if column is map.
            </summary>
            <param name="column">Column to apply</param>
            <param name="value">Value to locate</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArraySort(Microsoft.Spark.Sql.Column)">
            <summary>
            Sorts the input array in ascending order. The elements of the input array must
            be sortable. Null elements will be placed at the end of the returned array.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayRemove(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Remove all elements that equal to element from the given array.
            </summary>
            <param name="column">Column to apply</param>
            <param name="element">Element to remove</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayDistinct(Microsoft.Spark.Sql.Column)">
            <summary>
            Removes duplicate values from the array.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayIntersect(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns an array of the elements in the intersection of the given two arrays,
            without duplicates.
            </summary>s
            <param name="col1">Left side column to apply</param>
            <param name="col2">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayUnion(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns an array of the elements in the union of the given two arrays,
            without duplicates.
            </summary>
            <param name="col1">Left side column to apply</param>
            <param name="col2">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayExcept(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns an array of the elements in the `col1` but not in the `col2`,
            without duplicates. The order of elements in the result is nondeterministic.
            </summary>
            <param name="col1">Left side column to apply</param>
            <param name="col2">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Explode(Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a new row for each element in the given array or map column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ExplodeOuter(Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a new row for each element in the given array or map column.
            Unlike Explode(), if the array/map is null or empty then null is produced.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.PosExplode(Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a new row for each element with position in the given array or map column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.PosExplodeOuter(Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a new row for each element with position in the given array or map column.
            Unlike Posexplode(), if the array/map is null or empty then the row(null, null)
            is produced.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.GetJsonObject(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Extracts JSON object from a JSON string based on path specified, and returns JSON
            string of the extracted JSON object.
            </summary>
            <param name="column">Column to apply</param>
            <param name="path">JSON file path</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.JsonTuple(Microsoft.Spark.Sql.Column,System.String[])">
            <summary>
            Creates a new row for a JSON column according to the given field names.
            </summary>
            <param name="column">Column to apply</param>
            <param name="fields">Field names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromJson(Microsoft.Spark.Sql.Column,System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Parses a column containing a JSON string into a `StructType` or `ArrayType`
            of `StructType`s with the specified schema.
            </summary>
            <param name="column">Column to apply</param>
            <param name="schema">JSON format string or DDL-formatted string for a schema</param>
            <param name="options">Options for JSON parsing</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromJson(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Parses a column containing a JSON string into a `StructType` or `ArrayType`
            of `StructType`s with the specified schema.
            </summary>
            <param name="column">String column containing JSON data</param>
            <param name="schema">The schema to use when parsing the JSON string</param>
            <param name="options">Options for JSON parsing</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SchemaOfJson(System.String)">
            <summary>
            Parses a JSON string and infers its schema in DDL format.
            </summary>
            <param name="json">JSON string</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SchemaOfJson(Microsoft.Spark.Sql.Column)">
            <summary>
            Parses a JSON string and infers its schema in DDL format.
            </summary>
            <param name="json">String literal containing a JSON string.</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToJson(Microsoft.Spark.Sql.Column,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Converts a column containing a `StructType`, `ArrayType` of `StructType`s,
            a `MapType` or `ArrayType` of `MapType`s into a JSON string.
            </summary>
            <param name="column">Column to apply</param>
            <param name="options">Options for JSON conversion</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Size(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns length of array or map.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SortArray(Microsoft.Spark.Sql.Column,System.Boolean)">
            <summary>
            Sorts the input array for the given column in ascending (default) or
            descending order, the natural ordering of the array elements.
            </summary>
            <param name="column">Column to apply</param>
            <param name="asc">True for ascending order and false for descending order</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayMin(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the minimum value in the array.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayMax(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the maximum value in the array.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Shuffle(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a random permutation of the given array.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Reverse(Microsoft.Spark.Sql.Column)">
            <summary>
            Reverses the string column and returns it as a new string column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Flatten(Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a single array from an array of arrays. If a structure of nested arrays
            is deeper than two levels, only one level of nesting is removed.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sequence(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Generate a sequence of integers from `start` to `stop`, incrementing by `step`.
            </summary>
            <param name="start">Start expression</param>
            <param name="stop">Stop expression</param>
            <param name="step">Step to increment</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sequence(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Generate a sequence of integers from start to stop, incrementing by 1 if start is
            less than or equal to stop, otherwise -1.
            </summary>
            <param name="start">Start expression</param>
            <param name="stop">Stop expression</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayRepeat(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Creates an array containing the `left` argument repeated the number of times given by
            the `right` argument.
            </summary>
            <param name="left">Left column expression</param>
            <param name="right">Right column expression</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayRepeat(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Creates an array containing the `left` argument repeated the `count` number of times.
            </summary>
            <param name="left">Left column expression</param>
            <param name="count">Number of times to repeat</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MapKeys(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns an unordered array containing the keys of the map.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MapValues(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns an unordered array containing the values of the map.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MapFromEntries(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a map created from the given array of entries.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArraysZip(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a merged array of structs in which the N-th struct contains all
            N-th values of input arrays.
            </summary>
            <param name="columns">Columns to zip</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MapConcat(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the union of all the given maps.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``1(System.Func{``0})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``2(System.Func{``0,``1})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="A1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``3(System.Func{``0,``1,``2})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="A1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="A2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``4(System.Func{``0,``1,``2,``3})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="A1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="A2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="A3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``5(System.Func{``0,``1,``2,``3,``4})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="A1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="A2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="A3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="A4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``6(System.Func{``0,``1,``2,``3,``4,``5})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="A1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="A2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="A3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="A4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="A5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``7(System.Func{``0,``1,``2,``3,``4,``5,``6})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="A1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="A2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="A3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="A4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="A5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="A6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``8(System.Func{``0,``1,``2,``3,``4,``5,``6,``7})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="A1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="A2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="A3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="A4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="A5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="A6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="A7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``9(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="A1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="A2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="A3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="A4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="A5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="A6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="A7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="A8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``10(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="A1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="A2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="A3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="A4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="A5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="A6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="A7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="A8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="A9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>A delegate that when invoked will return a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``11(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9,``10})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="A1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="A2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="A3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="A4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="A5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="A6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="A7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="A8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="A9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="A10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <typeparam name="RT">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CallUDF(System.String,Microsoft.Spark.Sql.Column[])">
            <summary>
            Call an user-defined function registered via SparkSession.Udf().Register().
            </summary>
            <param name="udfName">Name of the registered UDF</param>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`1">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`2">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`3">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`4">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`5">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`6">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`7">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`8">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`9">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`10">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`11">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.RelationalGroupedDataset">
            <summary>
            A set of methods for aggregations on a DataFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.RelationalGroupedDataset.Agg(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column[])">
            <summary>
            Compute aggregates by specifying a series of aggregate columns.
            </summary>
            <param name="expr">Column to aggregate on</param>
            <param name="exprs">Additional columns to aggregate on</param>
            <returns>New DataFrame object with aggregation applied</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RelationalGroupedDataset.Count">
            <summary>
            Count the number of rows for each group.
            </summary>
            <returns>New DataFrame object with count applied</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Row">
            <summary>
            Represents a row object in RDD, equivalent to GenericRowWithSchema in Spark.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.#ctor(System.Object[],Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Constructor for the Row class.
            </summary>
            <param name="values">Column values for a row</param>
            <param name="schema">Schema associated with a row</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Row.Schema">
            <summary>
            Schema associated with this row.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Row.Values">
            <summary>
            Values representing this row.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.Size">
            <summary>
            Returns the number of columns in this row.
            </summary>
            <returns>Number of columns in this row</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Row.Item(System.Int32)">
            <summary>
            Returns the column value at the given index.
            </summary>
            <param name="index">Index to look up</param>
            <returns>A column value</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.Get(System.Int32)">
            <summary>
            Returns the column value at the given index.
            </summary>
            <param name="index">Index to look up</param>
            <returns>A column value</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.Get(System.String)">
            <summary>
            Returns the column value whose column name is given.
            </summary>
            <param name="columnName">Column name to look up</param>
            <returns>A column value</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.ToString">
            <summary>
            Returns the string version of this row.
            </summary>
            <returns>String version of this row</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.GetAs``1(System.Int32)">
            <summary>
            Returns the column value at the given index, as a type T.
            TODO: If the original type is "long" and its value can be
            fit into the "int", Pickler will serialize the value as int.
            Since the value is boxed, <see cref="M:Microsoft.Spark.Sql.Row.GetAs``1(System.Int32)"/> will throw an exception.
            </summary>
            <typeparam name="T">Type to convert to</typeparam>
            <param name="index">Index to look up</param>
            <returns>A column value as a type T</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.GetAs``1(System.String)">
            <summary>
            Returns the column value whose column name is given, as a type T.
            TODO: If the original type is "long" and its value can be
            fit into the "int", Pickler will serialize the value as int.
            Since the value is boxed, <see cref="M:Microsoft.Spark.Sql.Row.GetAs``1(System.String)"/> will throw an exception.
            </summary>
            <typeparam name="T">Type to convert to</typeparam>
            <param name="columnName">Column name to look up</param>
            <returns>A column value as a type T</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.Convert">
            <summary>
            Converts the values to .NET values. Currently, only the simple types such as
            int, string, etc. are supported (which are already converted correctly by
            the Pickler). Note that explicit type checks against the schema are not performed.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.RowCollector">
            <summary>
            RowCollector collects Row objects from a socket.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.RowCollector.Collect(Microsoft.Spark.Network.ISocketWrapper)">
            <summary>
            Collects pickled row objects from the given socket.
            </summary>
            <param name="socket">Socket the get the stream from</param>
            <returns>Collection of row objects</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.RowConstructor">
            <summary>
            RowConstructor is a custom unpickler for GenericRowWithSchema in Spark.
            Refer to spark/sql/core/src/main/scala/org/apache/spark/sql/execution/python/
            EvaluatePython.scala how GenericRowWithSchema is being pickeld.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.RowConstructor.s_currentSchema">
            <summary>
            Schema of the rows being received. Note that this is thread local variable
            because one RowConstructor object is registered to the Unpickler and there
            could be multiple threads unpickling the data using the same object registered.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.RowConstructor._values">
            <summary>
            Stores values passed from construct().
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.RowConstructor._schema">
            <summary>
            Stores the schema for a row.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.RowConstructor.ToString">
            <summary>
            Returns a string representation of this object.
            </summary>
            <returns>A string representation of this object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RowConstructor.construct(System.Object[])">
            <summary>
            Used by Unpickler to pass unpickled data for handling.
            </summary>
            <param name="args">Unpickled data</param>
            <returns>New RowConstructor object capturing unpickled data</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RowConstructor.GetRow">
            <summary>
            Construct a Row object from unpickled data.
            Resets the cached row schema.
            </summary>
            <returns>A row object with unpickled data</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.SaveMode">
            <summary>
            SaveMode is used to specify the expected behavior of saving a DataFrame to a data source.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.SaveMode.Append">
            <summary>
            Append mode means that when saving a DataFrame to a data source, if data/table already
            exists, contents of the DataFrame are expected to be appended to existing data.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.SaveMode.Overwrite">
            <summary>
            Overwrite mode means that when saving a DataFrame to a data source,
            if data/table already exists, existing data is expected to be overwritten by the
            contents of the DataFrame.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.SaveMode.ErrorIfExists">
            <summary>
            ErrorIfExists mode means that when saving a DataFrame to a data source, if data already
            exists, an exception is expected to be thrown.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.SaveMode.Ignore">
            <summary>
            Ignore mode means that when saving a DataFrame to a data source, if data already exists,
            the save operation is expected to not save the contents of the DataFrame and to not
            change the existing data.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.SparkSession">
            <summary>
            The entry point to programming Spark with the Dataset and DataFrame API.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Constructor for SparkSession.
            </summary>
            <param name="jvmObject">Reference to the JVM SparkSession object</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.SparkSession.SparkContext">
            <summary>
            Returns SparkContext object associated with this SparkSession.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Builder">
            <summary>
            Creates a Builder object for SparkSession.
            </summary>
            <returns>Builder object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Dispose">
            <summary>
            Synonym for Stop().
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.NewSession">
            <summary>
            Start a new session with isolated SQL configurations, temporary tables, registered
            functions are isolated, but sharing the underlying SparkContext and cached data.
            </summary>
            <remarks>
            Other than the SparkContext, all shared state is initialized lazily.
            This method will force the initialization of the shared state to ensure that parent
            and child sessions are set up with the same shared state. If the underlying catalog
            implementation is Hive, this will initialize the metastore, which may take some time.
            </remarks>
            <returns>New SparkSession object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Table(System.String)">
            <summary>
            Returns the specified table/view as a DataFrame.
            </summary>
            <param name="tableName">Name of a table or view</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Sql(System.String)">
            <summary>
            Executes a SQL query using Spark, returning the result as a DataFrame.
            </summary>
            <param name="sqlText">SQL query text</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Read">
            <summary>
            Returns a DataFrameReader that can be used to read non-streaming data in
            as a DataFrame.
            </summary>
            <returns>DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.ReadStream">
            <summary>
            Returns a DataStreamReader that can be used to read streaming data in as a DataFrame.
            </summary>
            <returns>DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Udf">
            <summary>
            Returns UDFRegistraion object with which user-defined functions (UDF) can 
            be registered.
            </summary>
            <returns>UDFRegistration object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Stop">
            <summary>
            Stops the underlying SparkContext.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Streaming.DataStreamReader">
            <summary>
            DataStreamReader provides functionality to load a streaming <see cref="T:Microsoft.Spark.Sql.DataFrame"/>
            from external storage systems (e.g. file systems, key-value stores, etc).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Format(System.String)">
            <summary>
            Specifies the input data source format.
            </summary>
            <param name="source">Name of the data source</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Schema(System.String)">
            <summary>
            Specifies the schema by using the given DDL-formatted string.
            </summary>
            <remarks>
            Some data sources (e.g. JSON) can infer the input schema automatically
            from data. By specifying the schema here, the underlying data source can
            skip the schema inference step, and thus speed up data loading.
            </remarks>
            <param name="schemaString">DDL-formatted string</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Option(System.String,System.String)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Option(System.String,System.Boolean)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Option(System.String,System.Int64)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Option(System.String,System.Double)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Options(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Adds input options for the underlying data source.
            </summary>
            <param name="options">Key/value options</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Load">
            <summary>
            Loads input data stream in as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>, for data streams
            that don't require a path (e.g.external key-value stores).
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Json(System.String)">
            <summary>
            Loads a JSON file stream and returns the results as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Csv(System.String)">
            <summary>
            Loads a CSV file stream and returns the result as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Orc(System.String)">
            <summary>
            Loads a ORC file stream and returns the result as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Parquet(System.String)">
            <summary>
            Loads a Parquet file stream and returns the result as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Text(System.String)">
            <summary>
            Loads text files and returns a <see cref="T:Microsoft.Spark.Sql.DataFrame"/> whose schema starts
            with a string column named "value", and followed by partitioned columns
            if there are any.
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.OptionInternal(System.String,System.Object)">
            <summary>
            Helper function to add given key/value pair as a new option.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.LoadSource(System.String,System.String)">
            <summary>
            Helper function to load the source for a given path.
            </summary>
            <param name="source">Name of the source</param>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Streaming.DataStreamWriter">
            <summary>
            DataStreamWriter provides functionality to write a streaming <see cref="T:Microsoft.Spark.Sql.DataFrame"/>
            to external storage systems (e.g. file systems, key-value stores, etc).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.OutputMode(System.String)">
            <summary>
            Specifies how data of a streaming DataFrame is written to a streaming sink.
            </summary>
            <remarks>
            The following mode is supported:
            "append": Only the new rows in the streaming DataFrame/Dataset will be written to
                      the sink.
            "complete": All the rows in the streaming DataFrame/Dataset will be written to the sink
                        every time there are some updates.
            "update": Only the rows that were updated in the streaming DataFrame will
                      be written to the sink every time there are some updates. If the query
                      doesn't contain aggregations, it will be equivalent to `append` mode.
            </remarks>
            <param name="outputMode">Output mode name</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.OutputMode(Microsoft.Spark.Sql.Streaming.OutputMode)">
            <summary>
            Specifies how data of a streaming DataFrame is written to a streaming sink.
            </summary>
            <param name="outputMode">Output mode enum</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Format(System.String)">
            <summary>
            Specifies the underlying output data source.
            </summary>
            <param name="source">Name of the data source</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Option(System.String,System.String)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Option(System.String,System.Boolean)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Option(System.String,System.Int64)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Option(System.String,System.Double)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Options(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Adds output options for the underlying data source.
            </summary>
            <param name="options">Key/value options</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Start(System.String)">
            <summary>
            Starts the execution of the streaming query.
            </summary>
            <param name="path">Optional output path</param>
            <returns>StreamingQuery object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.OptionInternal(System.String,System.Object)">
            <summary>
            Helper function to add given key/value pair as a new option.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Streaming.OutputMode">
            <summary>
            Output modes for specifying how data of a streaming DataFrame is written to a
            streaming sink.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.Streaming.OutputMode.Append">
            <summary>
            OutputMode in which only the new rows in the streaming DataFrame/Dataset will be
            written to the sink. This output mode can be only be used in queries that do not
            contain any aggregation.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.Streaming.OutputMode.Complete">
            <summary>
            OutputMode in which all the rows in the streaming DataFrame/Dataset will be written
            to the sink every time these is some updates. This output mode can only be used in
            queries that contain aggregations.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.Streaming.OutputMode.Update">
            <summary>
            OutputMode in which only the rows in the streaming DataFrame/Dataset that were updated
            will be written to the sink every time these is some updates. If the query doesn't
            contain aggregations, it will be equivalent to Append mode.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Streaming.StreamingQuery">
            <summary>
            A handle to a query that is executing continuously in the background as new data arrives.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Streaming.StreamingQuery.Name">
            <summary>
            Returns the user-specified name of the query, or null if not specified.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.IsActive">
            <summary>
            Returns true if this query is actively running.
            </summary>
            <returns>True if this query is actively running</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.AwaitTermination">
            <summary>
            Waits for the termination of this query, either by Stop() or by an exception.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.AwaitTermination(System.Int64)">
            <summary>
            Returns true if this query is terminated within the timeout in milliseconds.
            </summary>
            <remarks>
            If the query has terminated, then all subsequent calls to this method will either
            return true immediately (if the query was terminated by Stop()), or throw an
            exception immediately (if the query has terminated with an exception).
            </remarks>
            <param name="timeoutMs">Timeout value in milliseconds</param>
            <returns>true if this query is terminated within timeout</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.Stop">
            <summary>
            Stops the execution of this query if it is running. This method blocks until the
            threads performing execution stop.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.Explain(System.Boolean)">
            <summary>
            Prints the physical plan to the console for debugging purposes.
            </summary>
            <param name="extended">Whether to do extended explain or not</param>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.ArrayType">
            <summary>
            An array type containing multiple values of a type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.ArrayType.#ctor(Microsoft.Spark.Sql.Types.DataType,System.Boolean)">
            <summary>
            Constructor for ArrayType class.
            </summary>
            <param name="elementType">The data type of elements in this array</param>
            <param name="containsNull">Indicates if elements can be null</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.ArrayType.#ctor(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructor for ArrayType class.
            </summary>
            <param name="json">JSON object to create the array type from</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.ArrayType.ElementType">
            <summary>
            Returns the data type of the elements in an array.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.ArrayType.ContainsNull">
            <summary>
            Checks if the array can contain null values.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.ArrayType.SimpleString">
            <summary>
            Readable string representation for this type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.ArrayType.JsonValue">
            <summary>
            Returns JSON object describing this type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.ArrayType.FromJson(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructs a ArrayType object from a JSON object.
            </summary>
            <param name="json">JSON object used to construct a ArrayType object</param>
            <returns>ArrayType object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.MapType">
            <summary>
            The data type for a map. This class is not implemented yet.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.MapType.#ctor">
            <summary>
            Initializes the <see cref="T:Microsoft.Spark.Sql.Types.MapType"/> instance.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.StructField">
            <summary>
            A type that represents a field inside StructType.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructField.#ctor(System.String,Microsoft.Spark.Sql.Types.DataType,System.Boolean,Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructor for StructFieldType class.
            </summary>
            <param name="name">The name of this field</param>
            <param name="dataType">The data type of this field</param>
            <param name="isNullable">Indicates if values of this field can be null</param>
            <param name="metadata">The metadata of this field</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructField.#ctor(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructor for StructFieldType class.
            </summary>
            <param name="json">JSON object to construct a StructFieldType object</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructField.Name">
            <summary>
            The name of this field.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructField.DataType">
            <summary>
            The data type of this field.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructField.IsNullable">
            <summary>
            Checks if values of this field can be null.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructField.Metadata">
            <summary>
            The metadata of this field.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructField.ToString">
            <summary>
            Returns a readable string that represents this type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructField.JsonValue">
            <summary>
            Returns JSON object describing this type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.StructType">
            <summary>
            Struct type represents a struct with multiple fields.
            This type is also used to represent a Row object in Spark.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructType.#ctor(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructor for StructType class.
            </summary>
            <param name="json">JSON object to construct a StructType object</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructType.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Constructor for StructType class.
            </summary>
            <param name="jvmObject">StructType object on JVM</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructType.Fields">
            <summary>
            Returns a list of StructFieldType objects.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructType.#ctor(System.Collections.Generic.IEnumerable{Microsoft.Spark.Sql.Types.StructField})">
            <summary>
            Constructor for StructType class.
            </summary>
            <param name="fields">A collection of StructFieldType objects</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructType.SimpleString">
            <summary>
            Returns a readable string that represents this type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructType.JsonValue">
            <summary>
            Returns JSON object describing this type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructType.FromJson(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructs a StructType object from a JSON object
            </summary>
            <param name="json">JSON object used to construct a StructType object</param>
            <returns>A StuructType object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.DataType">
            <summary>
            The base type of all Spark SQL data types.
            Note that the implementation mirrors PySpark: spark/python/pyspark/sql/types.py
            The Scala version is spark/sql/catalyst/src/main/scala/org/apache/spark/sql/types/*.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.DataType.TypeName">
            <summary>
            Normalized type name.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.DataType.SimpleString">
            <summary>
            Simple string version of the current data type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.DataType.Json">
            <summary>
            The compact JSON representation of this data type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.DataType.JsonValue">
            <summary>
            JSON value of this data type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.ParseDataType(System.String)">
            <summary>
            Parses a JSON string to construct a DataType.
            </summary>
            <param name="json">JSON string to parse</param>
            <returns>The new DataType instance from the JSON string</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object by
            checking the string version of this type.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.ParseDataType(Newtonsoft.Json.Linq.JToken)">
            <summary>
            Parses a JToken object to construct a DataType.
            </summary>
            <param name="json">JToken object to parse</param>
            <returns>The new DataType instance from the JSON string</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.ParseSimpleType(Newtonsoft.Json.Linq.JToken)">
            <summary>
            Parses a JToken object that represents a simple type.
            </summary>
            <param name="json">JToken object to parse</param>
            <returns>The new DataType instance from the JSON string</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.NormalizeTypeName(System.String)">
            <summary>
            Remove "Type" from the end of type name and lower cases to align with Scala type name.
            </summary>
            <param name="typeName">Type name to normalize</param>
            <returns>Normalized type name</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.AtomicType">
            <summary>
            An internal type used to represent everything that is not null, arrays, structs, and maps.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.NumericType">
            <summary>
            Represents a numeric type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.IntegralType">
            <summary>
            Represents an integral type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.FractionalType">
            <summary>
            Represents a fractional type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.NullType">
            <summary>
            Represents a null type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.StringType">
            <summary>
            Represents a string type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.BinaryType">
            <summary>
            Represents a binary (byte array) type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.BooleanType">
            <summary>
            Represents a boolean type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.DateType">
            <summary>
            Represents a date type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.TimestampType">
            <summary>
            Represents a timestamp type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.DoubleType">
            <summary>
            Represents a double type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.FloatType">
            <summary>
            Represents a float type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.ByteType">
            <summary>
            Represents a byte type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.IntegerType">
            <summary>
            Represents an int type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.LongType">
            <summary>
            Represents a long type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.ShortType">
            <summary>
            Represents a short type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.DecimalType">
            <summary>
            Represents a decimal type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DecimalType.#ctor(System.Int32,System.Int32)">
            <summary>
            Initializes the <see cref="T:Microsoft.Spark.Sql.Types.DecimalType"/> instance.
            </summary>
            <remarks>
            Default values of precision and scale are from Scala:
            sql/catalyst/src/main/scala/org/apache/spark/sql/types/DecimalType.scala.
            </remarks>
            <param name="precision">Number of digits in a number</param>
            <param name="scale">
            Number of digits to the right of the decimal point in a number
            </param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.DecimalType.SimpleString">
            <summary>
            Returns simple string version of DecimalType.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.UdfRegistration">
            <summary>
            Functions for registering user-defined functions.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``1(System.String,System.Func{``0})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``2(System.String,System.Func{``0,``1})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``3(System.String,System.Func{``0,``1,``2})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``4(System.String,System.Func{``0,``1,``2,``3})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``5(System.String,System.Func{``0,``1,``2,``3,``4})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``6(System.String,System.Func{``0,``1,``2,``3,``4,``5})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``7(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``8(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``9(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``10(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``11(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9,``10})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.RegisterJava``1(System.String,System.String)">
            <summary>
            Register a Java UDF class using reflection.
            </summary>
            <typeparam name="TResult">Return type</typeparam>
            <param name="name">Name of the UDF</param>
            <param name="className">Class name that defines UDF</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.RegisterJavaUDAF(System.String,System.String)">
            <summary>
            Register a Java UDAF class using reflection.
            </summary>
            <param name="name">Name of the UDAF</param>
            <param name="className">Class name that defines UDAF</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``1(System.String,System.Delegate)">
            <summary>
            Helper function to register wrapped udf.
            </summary>
            <typeparam name="TResult">Return type of the udf</typeparam>
            <param name="name">Name of the udf</param>
            <param name="func">Wrapped UDF function</param>
        </member>
        <member name="T:Microsoft.Spark.Sql.WorkerFunction">
            <summary>
            Function that will be executed in the worker.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowWorkerFunction">
            <summary>
            Function that will be executed in the worker using the Apache Arrow format.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowWorkerFunction.ExecuteDelegate">
            <summary>
            Type of the UDF to run. Refer to <see cref="T:Microsoft.Spark.Sql.ArrowUdfWrapper`1"/>.Execute.
            </summary>
            <param name="splitId">split id for the current task</param>
            <param name="input">unpickled data, representing a row</param>
            <param name="argOffsets">offsets to access input</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.ArrowWorkerFunction.Chain(Microsoft.Spark.Sql.ArrowWorkerFunction,Microsoft.Spark.Sql.ArrowWorkerFunction)">
            <summary>
            Used to chain functions.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.ArrowWorkerFunction.WorkerFuncChainHelper.s_outerFuncArgOffsets">
            <summary>
            The outer function will always take 0 as an offset since there is only one
            return value from an inner function.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingWorkerFunction">
            <summary>
            Function that will be executed in the worker using the Python pickling format.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingWorkerFunction.ExecuteDelegate">
            <summary>
            Type of the UDF to run. Refer to <see cref="T:Microsoft.Spark.Sql.PicklingUdfWrapper`1"/>.Execute.
            </summary>
            <param name="splitId">split id for the current task</param>
            <param name="input">unpickled data, representing a row</param>
            <param name="argOffsets">offsets to access input</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.PicklingWorkerFunction.Chain(Microsoft.Spark.Sql.PicklingWorkerFunction,Microsoft.Spark.Sql.PicklingWorkerFunction)">
            <summary>
            Used to chain functions.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.PicklingWorkerFunction.WorkerFuncChainHelper.s_outerFuncArgOffsets">
            <summary>
            The outer function will always take 0 as an offset since there is only one
            return value from an inner function.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.Authenticator">
            <summary>
            Authenticator provides functionalities to authenticate between
            Spark and .NET worker.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Utils.Authenticator.AuthenticateAsClient(System.IO.Stream,System.String)">
            <summary>
            Authenticates by writing secret to stream and validate the response.
            </summary>
            <param name="stream">Valid stream.</param>
            <param name="secret">Secret string to authenticate against.</param>
            <returns>True if authentication succeeds.</returns>
        </member>
        <member name="M:Microsoft.Spark.Utils.Authenticator.AuthenticateAsServer(Microsoft.Spark.Network.ISocketWrapper,System.String)">
            <summary>
            Authenticates by reading secret from stream and writes the response code
            back to the stream.
            </summary>
            <param name="socket">Valid socket.</param>
            <param name="secret">Secret string to authenticate against.</param>
            <returns>True if authentication succeeds.</returns>
        </member>
        <member name="T:Microsoft.Spark.Utils.CommandSerDe">
            <summary>
            CommandSerDe provides functionality to serialize/deserialize WorkerFunction
            along with other information.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperMethodName">
            <summary>
            The function name of any UDF wrappers that wrap the UDF.
            ex) <see cref="M:Microsoft.Spark.RDD`1.MapUdfWrapper`2.Execute(System.Int32,System.Collections.Generic.IEnumerable{System.Object})"/>
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperNode">
            <summary>
            Captures the information about the UDF wrapper.
            Example classes for wrapping UDF are:
             - SQL: * <see cref="T:Microsoft.Spark.Sql.ArrowUdfWrapper`1"/>
                    * <see cref="T:Microsoft.Spark.Sql.PicklingUdfWrapper`1"/>
             - RDD: * <see cref="T:Microsoft.Spark.RDD`1.MapUdfWrapper`2"/>
                    * <see cref="T:Microsoft.Spark.RDD`1.FlatMapUdfWrapper`2"/>
                    * <see cref="T:Microsoft.Spark.RDD`1.MapPartitionsUdfWrapper`2"/>
                    * <see cref="T:Microsoft.Spark.RDD.WorkerFunction.WrokerFuncChainHelper"/>
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperNode.TypeName">
            <summary>
            Type name of the UDF wrapper.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperNode.NumChildren">
            <summary>
            Number of children (UDF wrapper or UDF) this node is associated with.
            Note that there can be up to two children and if the child is an UDF,
            this will be set to one.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperNode.HasUdf">
            <summary>
            True if the child is an UDF.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperData">
            <summary>
            UdfWrapperData represents the flattened tree structure.
            For example:
                                   WorkerChainHelper#1
                                    /                \
                      WorkerChainHelper#2        MapUdfWrapper#3
                          /          \                  \
             MapUdfWrapper#1   MapUdfWrapper#2         UDF#3
                    |                 |
                  UDF#1             UDF#2
            
            will be translated into:
            UdfWrapperNodes: (WorkerChainHelper(WCH), MapUdfWrapper(MUW))
               [ WCH#1(2, false), WCH#2(2, false), MUW#1(1, true), MUW#2(1, true), MUW#3(1, true) ]
               where WCH#1(2, false) means the node has two children and HasUdf is false.
            Udfs:
               [ UDF#1, UDF#2, UDF#3 ]
            
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperData.UdfWrapperNodes">
            <summary>
            Flattened UDF wrapper nodes.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperData.Udfs">
            <summary>
            Serialized UDF data.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.EnvironmentUtils">
            <summary>
            Various environment utility methods.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.PythonSerDe">
            <summary>
            Used for SerDe of Python objects.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Utils.PythonSerDe.GetUnpickledObjects(System.IO.Stream,System.Int32)">
            <summary>
            Unpickles objects from Stream.
            </summary>
            <param name="stream">Pickled byte stream</param>
            <param name="messageLength">Size (in bytes) of the pickled input</param>
            <returns>Unpicked objects</returns>
        </member>
        <member name="T:Microsoft.Spark.Utils.UdfSerDe">
            <summary>
            UdfSerDe is responsible for serializing/deserializing an UDF.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Utils.UdfSerDe.LoadAssembly(System.String)">
            <summary>
            Returns the loaded assembly by probing the following locations in order:
            1) The working directory
            2) The directory of the application
            If the assembly is not found in the above locations, the exception from
            Assembly.LoadFrom() will be propagated.
            </summary>
            <param name="manifestModuleName">The name of assembly to load</param>
            <returns>The loaded assembly</returns>
        </member>
        <member name="T:Microsoft.Spark.Utils.UdfTypeUtils">
            <summary>
            UdfTypeUtils provides fuctions related to UDF types.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Utils.UdfTypeUtils.CanBeNull(System.Type)">
            <summary>
            Returns "true" if the given type is nullable.
            </summary>
            <param name="type">Type to check if it is nullable</param>
            <returns>"true" if the given type is nullable. Otherwise, returns "false"</returns>
        </member>
        <member name="M:Microsoft.Spark.Utils.UdfTypeUtils.ImplementsGenericTypeOf(System.Type,System.Type)">
            <summary>
            Returns the generic type definition of a given type if the given type is equal
            to or implements the `compare` type. Returns null if there is no match.
            </summary>
            <param name="type">This type object</param>
            <param name="compare">Generic type definition to compare to</param>
            <returns>Matching generic type object</returns>
        </member>
        <member name="T:Microsoft.Spark.Utils.UdfUtils">
            <summary>
            UdfUtils provides UDF-related functions and enum.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.UdfUtils.PythonEvalType">
            <summary>
            Enum for Python evaluation type. This determines how the data will be serialized
            from Spark executor to its worker.
            Since UDF is based on PySpark implementation, PythonEvalType is used. Once
            generic interop layer is introduced, this will be revisited.
            This mirrors values defined in python/pyspark/rdd.py.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Utils.UdfUtils.s_returnTypes">
            <summary>
            Mapping of supported types from .NET to org.apache.spark.sql.types.DataType in Scala.
            Refer to spark/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala
            for more information.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Utils.UdfUtils.GetReturnType(System.Type)">
            <summary>
            Returns the return type of an UDF in JSON format. This value is used to
            create a org.apache.spark.sql.types.DataType object from JSON string.
            </summary>
            <param name="type">Return type of an UDF</param>
            <returns>JSON format of the return type</returns>
        </member>
        <member name="M:Microsoft.Spark.Utils.UdfUtils.CreatePythonFunction(Microsoft.Spark.Interop.Ipc.IJvmBridge,System.Byte[])">
            <summary>
            Creates the PythonFunction object on the JVM side wrapping the given command bytes.
            </summary>
            <param name="jvm">JVM bridge to use</param>
            <param name="command">Serialized command bytes</param>
            <returns>JvmObjectReference object to the PythonFunction object</returns>
        </member>
    </members>
</doc>
